---
title: "Hierarchical Cluster Analysis"
image: clustertutorial.webp
author:  
  - name: Jithin Chandran
    affiliations:
      - name: Statoberry LLP
    email: developer@statoberry.com
    note: Developer
    
  - name: Pratheesh P Gopinath
    affiliations:
      - name: Kerala Agricultural University
    email: pratheesh.pg@kau.in
    note: Mentor
    url: https://kau.in/people/sub-lt-dr-pratheesh-p-gopinath
  
  - name: Akhila P. S.
    affiliations:
      - name: Kerala Agricultural University
    note: tutorial contributor
    
format:
  html:
    mainfont: "Times New Roman"
    theme: yeti
    toc: true
    toc-depth: 3  
include-in-header:
  text: |
    <style>
      /* Hide the page-level description under the title (covers Quarto 1.4‚Äì1.5 DOMs) */
      #title-block-header .quarto-description,
      #title-block-header .description,
      header.quarto-title-block .quarto-description,
      header.quarto-title-block .description,
      header.quarto-title-block p.lead {
        display: none !important;
      }
    </style>
   
abstract: |  
  Hierarchical Cluster Analysis (HCA) is a statistical method is widely used in Agricultural research to group similar data points into clusters based on their characteristics, building a tree-like structure to show how these groups form (dendrogram). In RAISINS you can perform HCA very easily without writing a single line of code. This tutorial will guide you how to perform cluster analysis very easily in RAISINS and interpret the results effectively.  
  
description: |
  Hierarchical Cluster Analysis (HCA) widely used in Agricultural research ‚Ä¶
  <a href="/tutorial/cluster/Clustertutorial.html" style='color: grey; text-decoration: none;'>Read more ‚Ä¶</a>

metadata:
  description: null
   
categories: [Multivariate,Social-science]
date: 2025-11-01
reading-time: 10  
---

```{=html}
<style>
  sup {
    color: blue;
    font-size: 0.8em;  /* smaller superscript */
  }
  .affiliations {
    color: grey;
    font-size: 0.9em;  /* one size smaller than normal */
    margin-top: 0.2em;
  }
</style>  
```

*Below is a compact, practical guide you can use as a tutorial to perform Hierarchical Cluster Analysis in RAISINS.*

## Getting Started

**RAISINS (R and AI Solutions in INferential Statistics)** is a cloud-based platform that allows you to perform statistical analyses in R and Python without writing a single line of code. It runs entirely online which needs no downloads or installations and seamlessly integrates the capabilities of R, Python, and AI to deliver powerful yet user-friendly analytical tools.

Learn more about RAISINS [here](https://www.raisins.live/tutorial/Intro/raisinsIntro.html).

To get started with cluster analysis, visit [www.raisins.live](https://www.raisins.live), navigate to **Cluster Analysis** under [Data Analysis](https://www.raisins.live/#dataAnalysis) as shown in @fig-cluana.

![Cluster analysis in RAISINS](CLUSANA.webp){#fig-cluana fig-align="center" width="100%"}

## Hierarchical Cluster Analysis (HCA)

The purpose of hierarchical agglomerative cluster analysis is to group similar items or observations together based on how close or related they are. It helps you find natural groupings or patterns in your data without knowing the number of groups in advance.

In this method, the analysis starts with each item as its own separate group. Then, the most similar groups are joined step by step until all items come together into one big group. This process is shown using a tree-like diagram called a dendrogram, which helps you see how the groups were formed and decide where to ‚Äúcut‚Äù the tree to get the right number of clusters.

This method is useful when you want to explore your data and understand which items are more alike based on their features or values.

::: {.callout-tip title="In short"}
Hierarchical agglomerative cluster analysis helps group similar crops, varieties, or treatments based on their measured characters. The grouping is shown using a dendrogram, and since it is exploratory, users can decide how many clusters to keep - we will discuss finding the optimal number of clusters in the coming sessions.
:::

## Let's start with an example

We will explain hierarchical cluster analysis (HCA) using an example. Let‚Äôs consider a dataset with 30 genotypes and four variables - **Yield**, **Plant_Height**, **Pods_per_Plant**, and **Seed_Weight**. Our aim is to perform cluster analysis to group the genotypes based on these characters.

If you are from the social sciences, your goal will be similar, to group respondents or study units based on the variables under study. The arrangement of the data is shown in the @fig-modata. Data organized in the same way in MS Excel can be directly uploaded to **RAISINS** for analysis. For more details on data preparation see @sec-prepdata.

Two terms that we will use frequently are `Labels` and `Variables`. In our example, the Labels refer to the genotypes, and the Variables are the four traits mentioned earlier - Yield, Plant_Height, Pods_per_Plant, and Seed_Weight.

![Model dataset: 30 genotypes and four characters in MS Excel](modeldata.webp){#fig-modata fig-align="center"}

## How to Prepare Your Data? {#sec-prepdata}

Arranging data for upload in RAISINS is very simple. Prepare your data exactly like the one shown in @fig-modata, using a single-sheet Excel file. Make sure no blank rows are left above, and all columns have proper names. That‚Äôs it - your file is ready to upload.

Still if you have doubt read below:-

To prepare your dataset for analysis in RAISINS, you have two options:

<details>

<summary>Creating dataset in MS Excel</summary>

-   Open a new Microsoft Excel file, use single sheet only.

-   Start with Cell A1: begin entering data from cell A1. Do not leave any blank rows above.

-   First Row - Column Names: The first row must contain the column names.

-   Column 1: Enter treatment/labels. **There should not be any repetition in the label ID's**. If there is replications, you can use the mean values.

-   From Column 2 Onwards: Enter the names of each variable under study as separate columns (e.g.yield, Plant_Height, Pods_per_Plant, and Seed_Weight). You can give any names to the columns.

See @fig-modata showing how the prepared Excel file for upload should look like

If you have any doubt in saving a file as csv or some basics of data preperation read our tutorial on getting started [here](https://www.raisins.live/tutorial/Intro/raisinsIntro.html#sec-prepfile)

</details>

<details>

<summary>Creating your dataset directly within the RAISINS app</summary>

-   Navigate to `Create Data` Tab: Click on the Create Dataset tab in the main menu at the top of the app.

-   Specify Details: Enter the levels of Factor A, Factor B, Factor C and number of characters under study in the window that opens.

-   Then click `create`

-   Model Data Entry File: A template for data entry will be generated. You can:

    -   Directly enter your data into this template.

    -   Or, copy-paste data from an existing Excel file.

-   Download as CSV: Once the data is entered, click on the `Download CSV` File button. The downloaded CSV file can be uploaded for analysis in `Analysis tab`.

![Creating Data in RAISINS](create.webp){#fig-cr fig-align="center" width="100%"}

</details>

## Running Analysis

Now upload the prepared file by clicking `Browse` in the sidebar of the `Analysis tab` (see @fig-analtab).

![Analysis tab](anal_tab.webp){#fig-analtab fig-align="center" width="100%"}

When the file is uploaded, options to select the variables and labels appear. Select appropriate column under Labels and variables, see @fig-upload

![Uploading the data file](upload.webp){#fig-upload fig-align="center" width="100%"}

Once you click the `Run Analysis` button, all relevant results and outputs appear instantly-leaving no room for confusion. Now see the result for our example data in the form of a dendrogram @fig-result.

::: {.callout-note title="Dendrogram"}
A dendrogram is a tree-like diagram that visually represents how hierarchical clustering groups data step by step, allowing you to explore relationships and cluster formation within a dataset. A dendrogram displays the process of merging (or sometimes splitting) clusters during hierarchical cluster analysis\

<details>

<summary>Step-by-Step: How to read a Dendrogram ?</summary>

1.  **Start at the bottom** - Each leaf represents a single observation or sample.

2.  **Follow the lines upward** - The first merges connect the most similar items. These connections indicate that the points or small clusters joined are very close based on the chosen similarity measure.

3.  **Merging clusters** - As you go higher, clusters are combined with others, and each branching indicates additional clustering - think of it as building a family tree for your data.

4.  **Branch heights** - The height at which branches merge tells you how similar the merged groups are-the lower the connection, the more similar the groups.

5.  **Deciding number of clusters** - By ‚Äúcutting‚Äù the tree horizontally at a given level (distance threshold), you divide the data into clusters-each branch below the cut line forms a cluster.

</details>
:::

![Dendrogram of the model dataset](result.webp){#fig-result fig-align="center" width="100%"}

In RAISINS, the optimal number of clusters is automatically identified using the `elbow method`.Other options can be seen in `optimal clusters` tab. In this example, the method suggests two clusters, which are shown in red and blue in @fig-result.

You can also change the number of clusters as needed. RAISINS provides many additional options that the experimenter can choose from depending on the type of data and research objective. The platform is highly interactive and guides you on what to use and when, making the clustering process easier and more intuitive, see @sec-detail.

## RAISINS clustering options explained {#sec-detail}

In @fig-Run, you can see the detailed view of the Analysis tab, along with explanations of what each option does. This section helps you understand the purpose of every setting so you can select the most appropriate ones for your data and analysis needs.

![Run Analysis Tab in RAISINS](AnalysisRun.webp){#fig-Run fig-align="center" width="100%"}

### Different plot types

RAISINS offers a variety of plots related to Hierarchical Cluster Analysis, ensuring that all key visualizations are easily accessible. Each plot includes a gear icon at the top-left corner that lets you customize its appearance to suit your needs. You can also download these plots in high-quality PNG (300 dpi), TIFF, or PDF formats for use in your reports or presentations (see @fig-Run).

From @fig-1 to @fig-8, you can see the different types of dendrograms available in RAISINS. Each one is visually illustrated and accompanied by a clear, insightful description below, making it easy to understand the structure, grouping, and interpretation of each clustering pattern.

::: {layout-ncol="2"}
![**Color coded clusters (k = 2)**\
A color-coded cluster dendrogram is an enhanced version of the basic dendrogram where branches or clusters are colored differently to clearly distinguish the groupings formed by hierarchical clustering. This makes interpreting the tree much easier, especially for large datasets or when clusters overlap visually.](Color_coded.webp){#fig-1 width="100%"}

![**Vertical Dendrogram (k = 2)**\
A vertical dendrogram is a classic, user-friendly way to display hierarchical clustering results with the data points along the bottom and cluster merges represented by upward vertical lines, enabling easy interpretation of cluster hierarchy and distances.](Vertical_Dendrogram.webp){#fig-2 width="100%"}
:::

::: {layout-ncol="2"}
![**Horizontal Dendrogram (k = 2)**\
A horizontal dendrogram is a tree-like diagram used to visualize hierarchical clustering results where the structure extends horizontally rather than vertically. In this layout, the individual observations or samples appear along the vertical axis, and the branches stretch left to right, representing how clusters merge step-by-step based on dissimilarity or distance.](Horizontal_Dendrogram.webp){#fig-3 width="100%"}

![**Ward's Linkage (k = 2)**\
Ward's linkage is a hierarchical clustering method focused on minimizing the increase in total within-cluster variance (or error sum of squares) when merging clusters. It is widely used because it tends to produce compact, spherical clusters, making it popular in many applications like biological data, market segmentation, and more.](Wards_Linkage.webp){#fig-4 width="100%"}
:::

::: {layout-ncol="2"}
![**Rectangular Clusters (k = 2)**\
Rectangular clusters are simply the visual aids drawn as boxes around groups of points identified as clusters on dendrograms or heat maps to facilitate clear, intuitive interpretation of cluster structure in hierarchical clustering outputs.](Rectangular_Clusters.webp){#fig-5 width="100%"}

![**Circular dendrogram (k = 2)**\
A Circular Dendrogram is a radial, space-efficient alternative to the classical dendrogram, arranging hierarchical cluster branches in concentric circles around a center. It enhances visual clarity for large or complex datasets by providing a 360-degree view of similarity and cluster structures, making it a valuable tool for hierarchical data visualization.](Circular_dendrogram.webp){#fig-6 width="100%"}
:::

::: {layout-ncol="2"}
![**Colored Rectangular Dendrogram (k = 2)**\
A coloured rectangular dendrogram is an enhanced dendrogram visualization where clusters are not only enclosed in rectangular boxes but these boxes and the associated branches are also color-coded to clearly distinguish different clusters. This combination improves interpretability and presentation of hierarchical clustering. results.](Colored_Rectangular_Dendrogram.webp){#fig-7 fig-align="center" width="100%"}

![**Classical Dendrogram (k = 2)** A classical dendrogram is the traditional tree-like diagram used in hierarchical cluster analysis to represent the nested grouping of objects based on their similarity or distance. It is a fundamental visualization tool that shows how clusters are formed progressively, starting from each individual object and merging step by step into larger clusters until all objects are grouped into one.\
](Classical_Dendrogram.webp){#fig-8 fig-align="center" width="100%"}
:::

In addition to the various plot types available in RAISINS, the platform also provides several customization options to refine your clustering analysis (See @fig-Run)

You can explore and adjust the following settings:

### Linkage Methods

Following linkage methods are available in RAISINS - single, complete, average, ward.D2, mcquitty, median and centroid.

<details>

<summary>Lets understand the different linkage methods</summary>

::: {.callout-note title="üîπ Single linkage" collapse="false"}
Single linkage defines the distance between two clusters as the minimum distance between any pair of points, one from each cluster. This approach often produces elongated, chain-like clusters because it merges clusters based on the closest individual pair of points. It is sensitive to noise and outliers, as a single close pair can link otherwise distant clusters.
:::

::: {.callout-note title="üîπ Complete linkage" collapse="false"}
Complete linkage measures the distance between two clusters as the maximum distance between any point in one cluster and any point in the other cluster. This method tends to produce more compact and spherical clusters by enforcing that all members within a cluster are close to each other. It is less sensitive to outliers compared to single linkage.
:::

::: {.callout-note title="üîπ Average linkage" collapse="false"}
Average linkage computes the clustering distance as the average of all pairwise distances between members of the two clusters. It balances the tendency of single and complete linkage, typically yielding clusters that are more balanced in shape and size by considering all members evenly.
:::

::: {.callout-note title="üîπ Ward.D2 Linkage" collapse="false"}
Ward‚Äôs linkage merges clusters such that the increase in total within-cluster variance is minimized. It uses a variance-minimizing criterion based on squared Euclidean distances and tends to produce compact, spherical clusters. Ward.D2 specifically refers to Ward's method with squared distances, popular for its stable and interpretable clusters.
:::

::: {.callout-note title="üîπ McQuitty Linkage" collapse="false"}
This method uses a simple average of distances weighted by cluster sizes when merging clusters. It‚Äôs a variant of average linkage and provides a compromise that accounts for different cluster sizes during the merge.
:::

::: {.callout-note title="üîπ Median Linkage" collapse="false"}
Median linkage calculates the distance between clusters based on the median of the points within each cluster (cluster median) rather than the mean. This method can be more robust to skewed data distributions but may result in non-monotonic clustering (reversals) in dendrograms.
:::

::: {.callout-note title="üîπ Centroid Linkage" collapse="false"}
Centroid linkage computes the distance between two clusters as the Euclidean distance between their centroids (mean vectors). It effectively merges clusters based on the center of mass but may produce inversions (discontinuities) in the dendrogram if the merging decreases cluster distances temporarily..
:::

</details>

### Distance Metrics

Distance metrics options available in RAISINS: Euclidean, Manhattan, Maximum, Canberra, Minkowski.

<details>

<summary>Lets understand the different Distance Metrics</summary>

::: {.callout-note title="üîπ Euclidean Distance" collapse="false"}
Euclidean distance is the most familiar form of distance measurement, representing the straight-line distance between two points in Euclidean space. It is calculated using the Pythagorean theorem, where the distance between two points is the square root of the sum of the squared differences across all dimensions. For example, in a two-dimensional space, the distance between points (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ) is given by the formula ‚àö((x‚ÇÇ ‚àí x‚ÇÅ)¬≤ + (y‚ÇÇ ‚àí y‚ÇÅ)¬≤). This metric measures the shortest path between points, like stretching a string tight between two points on a map, making it widely used in machine learning, such as clustering and classification algorithms.
:::

::: {.callout-note title="üîπ Manhattan Distance" collapse="false"}
Manhattan distance, also known as city block distance, sums the absolute differences of the coordinates between two points in a grid-like path. Unlike Euclidean distance, which measures the shortest "as-the-crow-flies" route, Manhattan distance measures how far apart two points are if you can only move along grid lines‚Äîlike navigating city streets laid out in a grid pattern. For example, the distance between (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ) is \|x‚ÇÇ ‚àí x‚ÇÅ\| + \|y‚ÇÇ ‚àí y‚ÇÅ\|, often used in urban planning and in applications where movement is restricted to axes-aligned paths.
:::

::: {.callout-note title="üîπ Maximum Distance" collapse="false"}
Maximum distance, also known as Chebyshev distance, considers the greatest absolute difference among all coordinate pairs between two points. It effectively measures how far apart two points are in terms of the most significant coordinate difference. Mathematically, for points (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ), it is max(\|x‚ÇÇ ‚àí x‚ÇÅ\|, \|y‚ÇÇ ‚àí y‚ÇÅ\|). This metric is useful in chess (king's move), robotics (max step size), and in certain clustering methods, where the largest coordinate gap dominates the distance calculation.
:::

::: {.callout-note title="üîπ Canberra Distance" collapse="false"}
Canberra distance is a weighted version of difference measurement that emphasizes smaller differences, especially when values are close to zero. It is calculated as the sum of the ratios \|x‚ÇÇ ‚àí x‚ÇÅ\| / (\|x‚ÇÇ\| + \|x‚ÇÅ\|) across all dimensions. Because it gives more weight to differences when values are small, it is effective for datasets with many small or sparse values, often used in bioinformatics, economics, and fields dealing with relative differences.
:::

::: {.callout-note title="üîπ Minkowski Distance" collapse="false"}
Minkowski distance is a generalization of Euclidean and Manhattan distances, characterized by a parameter p, called the order. When p=1, Minkowski becomes Manhattan distance; when p=2, it becomes Euclidean distance. For other values of p, it defines a different form of distance calculation, where higher p emphasizes larger differences more heavily, and lower p emphasizes smaller differences. The formula involves taking the p-th root of the sum of the absolute differences raised to the power p across all dimensions, making it very flexible for various applications.
:::

::: {.callout-note title="üîπ Median and Centroid" collapse="false"}
Median distance refers to the measure of the difference between data points based on their median values; it is more robust to outliers in skewed data distributions. Centroid distance specifically considers the Euclidean distance between the centroids (mean points) of data clusters. The centroid-based method is widely used in clustering techniques like k-means because it measures the central point of a cluster, making it useful for cluster center-based algorithms but sensitive to outliers, which can shift the centroid.
:::

</details>

### Methods to find the optimal number of clusters (k)

In RAISINS, three methods are available to determine the optimal number of clusters - Elbow, Silhouette, and Gap Statistic..

<details>

<summary>Lets understand the different methods to find optimal number of clusters</summary>

::: {.callout-note title="üîπ  Elbow Method" collapse="false"}
The elbow method is a heuristic used in clustering analysis, particularly with K-means, to determine the optimal number of clusters. It involves plotting the within-cluster sum of squares (WCSS) against different values of k (number of clusters). As the number of clusters increases, WCSS decreases because the data points within each cluster become more tightly grouped. The goal is to identify the "elbow" point in the plot where the rate of decrease sharply changes, indicating an optimal balance between minimizing intra-cluster variance and avoiding overfitting. This point suggests the most appropriate number of clusters, where adding more clusters yields diminishing improvements.
:::

::: {.callout-note title="üîπ  Silhouette Method" collapse="false"}
The Silhouette method measures how similar an object is to its own cluster compared to other clusters. For each data point, the Silhouette score ranges from -1 to 1, with higher scores indicating better clustering (points are closer to members of their own cluster than to other clusters). The average silhouette score across all points for different cluster numbers helps identify the optimal k: the higher the average score, the better the clustering. This method considers both cohesion within clusters and separation between clusters, providing a more nuanced evaluation of clustering quality than solely looking at variance.
:::

::: {.callout-note title="üîπ  Gap Statistic" collapse="false"}
The Gap Statistic compares the total within-cluster dispersion for different k values with their expected dispersion under a null reference distribution (usually a uniform distribution of points). It calculates the gap between the observed clustering and the expected clustering under randomness: a larger gap indicates more meaningful, well-separated clusters. The optimal number of clusters is the one where the gap statistic reaches its maximum, suggesting that the clustering structure differs significantly from random noise. This method is robust but more computationally intensive, as it involves generating multiple reference datasets for comparison.
:::

</details>

::: {layout-ncol="3"}
![**Elbow plot**](Elbowplot.webp){#fig-ep width="100%"}

![**Silhouette plot**](Silhouetteplot.webp){#fig-sp width="100%"}

![**Gap Statistic**](GapStatistic.webp){#fig-gs width="100%"}
:::

##### **Scaling Options available in raisins: zscore, center, minmax, unitlength, robust, none**.

<details>

<summary>Lets understand the different scaling options</summary>

::: {.callout-note title="üîπ  zscore" collapse="false"}
Z-score normalization, also known as standardization, transforms data so that it has a mean of 0 and a standard deviation of 1. This method computes the Z-score for each data point by subtracting the dataset's mean (Œº) and dividing by its standard deviation (œÉ). The resulting scaled data indicates how many standard deviations each point is from the mean. It is particularly useful for datasets with roughly normal distributions and algorithms that assume normality, such as linear regression or SVM, as it removes scale bias and enhances comparability across features.
:::

::: {.callout-note title="üîπ  Center" collapse="false"}
Centering data refers to subtracting the mean of a feature from each data point, effectively shifting the data so the mean becomes zero without changing the data's variance or distribution shape. This operation is fundamental to many data preprocessing steps, such as Z-score normalization, and helps algorithms interpret features more straightforwardly by aligning data around a common point, making the analysis less affected by location shifts.
:::

::: {.callout-note title="üîπ  MinMax" collapse="false"}
Min-Max scaling rescales data to a fixed range, typically. It adjusts each feature by subtracting the minimum value and dividing by the range (max - min), effectively compressing all data within the specified bounds. This technique preserves the original distribution but is highly sensitive to outliers, which can distort the scaled values by expanding the range. Min-Max normalization is often used in neural networks and gradient-based algorithms where feature bounds matter.
:::

::: {.callout-note title="üîπ  Unit Length" collapse="false"}
Unit length scaling normalizes each data sample (or feature vector) so that its total length (or Euclidean norm) equals 1. This is achieved by dividing each vector by its magnitude, which is the square root of the sum of squared components. It‚Äôs commonly used in text analysis (e.g., TF-IDF vectors) and machine learning algorithms that rely on cosine similarity, as it ensures all vectors are on the same scale, emphasizing direction over magnitude.
:::

::: {.callout-note title="üîπ  Robust" collapse="false"}
Robust scaling uses statistics less sensitive to outliers, typically by subtracting the median and dividing by the interquartile range (IQR). This method centers the data around the median and scales it according to the spread of the middle 50% of data points, making it effective when dealing with noisy data or datasets with extreme outliers. Its primary goal is to produce scaled data that reflects the true distribution of the bulk of the data without being skewed by outliers.
:::

</details>

### Heatmap in RAISINS

A heatmap in cluster analysis is a visualization technique that represents data values in a matrix format using colors to indicate magnitude, typically after both the rows and columns have been reordered based on hierarchical clustering results. The integration of heatmaps with cluster analysis allows you to visually explore and identify patterns, groupings, or similarities among observations (rows) and variables (columns).

In the RAISINS platform after uploading your data click on the `Heatmap tab` to generate the heatmap and you can use the gear icon to customize your heatmap (See @fig-hptab) By default the data are standardized using the `zscore` scaling method, ensuring all features contribute equally to clustering, which was performed using the `complete` linkage method and `euclidean` distance metric, resulting in `2` clusters.You can also download the heatmapin high-quality PNG (300 dpi), TIFF or PDF formats for use in reports or presentations.

![**How to access the Heatmap tab in RAISINS**](Heatmaptab.webp){#fig-hptab fig-align="center" width="75%"}

See @fig-hp where blocks of similar colors indicate labels or variables with related profiles, and the accompanying dendrograms illustrate their hierarchical relationships, making it easier to identify groups that share similar characteristics or expression patterns.

![**Heatmap in RAISINS**](heatmap.webp){#fig-hp fig-align="center" width="100%"}

### Metrics tab

In `Metrics` (See @fig-adm) tab you can see the Distance matrix, Cluster-wise means, Intra-cluster Statistics and Inter-cluster Statistics tables and you can either copy this file or download these tables in `.xlsx` or `.csv` formats for use in reports or presentations.

![**Metrics tab in RAISINS**](mettab.webp){#fig-adm fig-align="center" width="100%"}

Different metrics available in Metric tab is given below.

1.  Distance Matrix (euclidean method) - See @fig-dm

![**Distance Matrix in RAISINS**](distance.webp){#fig-dm fig-align="center" width="100%"}

In hierarchical clustering, the distance matrix represents the pairwise dissimilarities or distances between all observations in the dataset. It quantifies how similar or different each observation is from every other observation and serves as the foundation for grouping observations into clusters. The distance matrix provides a reference for understanding the relative closeness of observations and guides the formation of clusters based on similarity.

2.  Cluster means - See @fig-cwm

![**Cluster wise means in RAISINS**](clust_means.webp){#fig-cwm fig-align="center" width="100%"}

In hierarchical clustering, the cluster mean represents the average values of all variables for the observations within a cluster, summarizing the central tendency of the cluster and providing a reference point to understand the characteristics of the grouped data. It represents the typical characteristics of that cluster.

3.  Intra-cluster Statistics - See @fig-itra

![**Intracluster statistics**](intra.webp){#fig-itra fig-align="center" width="100%"}

In hierarchical clustering, intra-cluster statistics measure the compactness or similarity of observations within the same cluster. They quantify how closely the members of a cluster resemble each other, often using metrics like the average distance between observations and the cluster mean. These statistics help assess the cohesion of a cluster, indicating how homogeneous or tightly grouped the cluster members are.

4.  Inter-cluster Statistics - See @fig-iter

![**Intercluster statistics**](inter.webp){#fig-iter fig-align="center" width="100%"}

In hierarchical clustering, inter-cluster statistics measure the separation or dissimilarity between different clusters. They quantify how distinct or far apart the clusters are from each other, often using metrics like the distance between cluster means or centroids. These statistics help assess the distinctness of clusters, indicating how well the clustering algorithm has separated different groups in the dataset.

### HCPC (Hierarchical Clustering on Principal Components)

Hierarchical clustering on principal components (HCPC) is a hybrid approach that combines the strengths of principal component analysis (PCA) and hierarchical clustering. PCA first reduces the dimensionality of the dataset by summarizing correlated variables into a smaller set of uncorrelated components that retain most of the original variance. Then, hierarchical clustering is performed on these principal components instead of the raw variables, which enhances the separation of clusters and reduces noise caused by variable correlations. This method is particularly useful for visualizing and identifying meaningful groupings in complex multivariate datasets, as it simplifies interpretation while preserving the main structure of the data.

<details>

<summary>When to use HCPC ?</summary>

-   When you have a large dataset with many continuous variables and want to reduce dimensionality before clustering to improve interpretability and cluster quality.

-   When your dataset contains categorical variables, multiple correspondence analysis (MCA) or factor analysis can transform them into continuous principal components suitable for clustering.

-   When original variables are correlated or noisy, HCPC clusters on principal components that summarize the main variance structure and filter noise, leading to more meaningful and stable clusters.

-   When you want an integrated approach combining principal component analysis for dimensionality reduction, hierarchical clustering for initial cluster identification, and k-means clustering for refinement.

-   When exploring multivariate data for identifying natural groupings, patterns, or clusters with clearer separation and visual representation (e.g., factor maps, chord diagrams).

-   When the goal is simplifying complex multivariate data for easier interpretation and presentation in reports or scientific communication.

</details>

<details>

<summary>Difference between HCPC and classical HCA</summary>

-   HCPC applies Principal Component Analysis (PCA) first to reduce data dimensionality before clustering; HCA clusters directly on the original variables.

-   HCPC clusters the uncorrelated principal components that represent the main variance structure, filtering noise and redundancy; HCA uses raw data and can be influenced by correlated variables.

-   HCPC integrates PCA, hierarchical clustering, and k-means clustering to improve cluster stability and quality; HCA only involves hierarchical clustering.

-   HCPC is designed for complex, high-dimensional, or mixed-type datasets; HCA is suitable for simpler, low-dimensional data.

-   HCPC produces enhanced visualizations like factor maps and chord diagrams for better cluster interpretation; HCA mainly produces dendrograms based on distances.

-   HCPC requires additional computational steps but offers more robust and interpretable clusters; HCA is simpler and faster but may lead to less meaningful clusters in complex data.

</details>

In the RAISINS platform after uploading your data `select the labels` then `select the clustering variables` of your choice, then click on `Run Analysis` and go to `HCPC` to generate Factor map, Chord diagram and Chord diagram - Cluster means which are downloadable in PNG, TIFF or PDF formats with interpretation and tables. The tables can be either copied or downloaded in `.xlsx` or `.csv` formats for use in reports or presentations.

::: {layout-ncol="3"}
![**Factor Map**](Factor_map.webp){#fig-facmap width="100%"}

![**Chord diagram**](Chord.webp){#fig-chord width="100%"}

![**Chord diagram - Cluster means**](Means_Chord.webp){#fig-chordmean width="100%"}
:::

In the @fig-facmap the Factor map displays the results of Hierarchical Clustering on Principal Components (HCPC), where observations are grouped into distinct clusters based on their similarity across all variables. The plot is constructed in a reduced two-dimensional space using the first two principal components (Dim1 and Dim2), which capture the maximum variance in the data while preserving the essential structure of the original multidimensional dataset. Each cluster is represented by a different color and enclosed in a convex hull (shaded polygon), making it easy to visually distinguish between groups. Observations within the same cluster share similar characteristics across the measured variables, while observations in different clusters exhibit distinct patterns or profiles. The percentages shown on each axis indicate how much of the total variance in the data is explained by that dimension - higher percentages mean that dimension captures more information about the differences between observations. The spatial separation between clusters reflects how different they are: clusters that are far apart have very different profiles, while clusters that are closer together are more similar. Overlapping regions suggest some ambiguity in cluster boundaries, where observations share characteristics with multiple groups. The position of each observation (data point label) within the plot is determined by its scores on the principal components, allowing you to see not only which cluster each observation belongs to but also how it relates to other observations within and across clusters. This visualization helps identify natural groupings in your data, understand the relationships between observations, and assess the quality of the clustering by examining how well-separated and cohesive the clusters appear in the reduced dimensional space

![**Top variables**](Topvariables%20.webp){#fig-topvar fig-align="center" width="100%"}

In @fig-topvar, the table shows the top contributing variables for each cluster based on the v.test statistic from the HCPC analysis. For each cluster, the five variables with the highest absolute v.test values are displayed. 'Mean in category' and 'sd in category' show the standardized mean and standard deviation within the cluster, while 'Overall mean' and 'Overall sd' provide reference values across all data. The p.value indicates the statistical significance of the variable's contribution. Variables with higher absolute v.test values and lower p.values are more important in defining the cluster.

In @fig-chord, the chord diagram visualizes the relationships between your original variables and the identified clusters from Hierarchical Clustering on Principal Components (HCPC). The outer ring shows two types of segments: your original dataset variables and the clusters (groups of similar observations). The ribbons connecting them represent the strength of association, with wider ribbons indicating that a variable is more characteristic or defining for that particular cluster. The width is determined by v.test statistics, which measure how strongly a variable distinguishes each cluster from others. By examining these connections, you can understand the profile of each cluster - which variables are most prominent in defining each group. Variables with thick ribbons to a cluster are key features of that group, while variables connected to multiple clusters play important roles across different groups. The diagram displays the most characteristic variables for each cluster with weights normalized within each cluster, allowing you to compare the relative importance of variables in defining cluster identities and understand what makes each group unique in your dataset.

::: {layout-ncol="2"}
![**HCPC Dendrogram**](hcpc_dendro.webp){#fig-hcpc_dendro width="100%" height="250"}

![**Heirarchical Factor Map**](hcpc_3d.webp){#fig-hcpc_3d width="100%" height="250"}
:::

In @fig-hcpc_dendro, the hierarchical dendrogram obtained through Hierarchical Clustering on Principal Components (HCPC) provides a visual representation of how observations are grouped based on their similarity in the principal component space. The vertical branches depict the progressive merging of individuals or groups, where shorter branches indicate closer similarity and longer branches represent greater dissimilarity between clusters. The height of each merge on the y-axis corresponds to the distance or dissimilarity at which clusters combine, allowing you to assess how distinct each group is. The colored rectangles highlight the main clusters identified by the analysis, where observations within the same color share similar multivariate characteristics derived from PCA. By examining the branching pattern and the height at which groups join, you can interpret the overall structure and relationships among the observations‚Äîhow closely related they are, which clusters are more homogeneous, and where clear separations exist. This dendrogram thus serves as a hierarchical map of similarity, illustrating the underlying grouping patterns in your dataset as summarized by the principal components.

In @fig-hcpc_3d, the three-dimensional hierarchical clustering plot projected onto the PCA factor map visually integrates both the dimensional reduction from Principal Component Analysis (PCA) and the grouping structure from Hierarchical Clustering on Principal Components (HCPC). The horizontal plane represents the first two principal components (Dim 1 and Dim 2), which together capture the largest proportion of variance in the dataset, while the vertical axis (‚Äúheight‚Äù) indicates the dissimilarity or distance at which observations or clusters are merged. Each line segment shows how closely related individuals (such as genotypes or samples) are in the multivariate space; shorter vertical distances signify stronger similarity. The clusters, represented by colored labels, highlight groups of observations that share common patterns along the principal component dimensions. This 3D representation allows you to observe not only how clusters are hierarchically formed but also how they are spatially positioned and separated in the PCA space, providing a comprehensive view of both the clustering process and the underlying data structure derived from the principal components.

![**PCA Summary Table**](clus_sum.webp){#fig-pca width="100%"}

In @fig-pca, the table summarizes the results of Principal Component Analysis (PCA), showing how much variance each principal component explains in the dataset. The eigenvalue indicates the amount of variance captured by each component, with larger values representing more important components. The percent column shows the proportion of total variance explained by each component, while the cumulative percent shows the running total across components. Components with eigenvalues greater than 1 are typically considered meaningful. This information helps determine how many components are needed to adequately represent the original data and forms the basis for subsequent clustering analysis.

![**Cluster quality indicators**](ClusQua.webp){#fig-clusqua width="100%"}

In @fig-clusqua, the table summarizes the variance in the dataset with respect to clustering. Total Sum of Squares (Total_SS) represents the overall variability in the data. Within-Cluster Sum of Squares (Within_SS) measures variability of observations inside each cluster. Between-Cluster Sum of Squares (Between_SS) quantifies variability between cluster centers. The Between/Total Ratio indicates the proportion of total variance explained by the differences between clusters. Higher ratios mean better separation between clusters.

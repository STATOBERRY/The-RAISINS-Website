[
  {
    "objectID": "tutorial/cluster/Clustertutorial.html",
    "href": "tutorial/cluster/Clustertutorial.html",
    "title": "Hierarchical Cluster Analysis",
    "section": "",
    "text": "Below is a compact, practical guide you can use as a tutorial to perform Hierarchical Cluster Analysis in RAISINS."
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#getting-started-in-raisins",
    "href": "tutorial/cluster/Clustertutorial.html#getting-started-in-raisins",
    "title": "Hierarchical Cluster Analysis",
    "section": "1 Getting Started in RAISINS",
    "text": "1 Getting Started in RAISINS\nRAISINS (R and AI Solutions in INferential Statistics) is an online platform that simplifies data analysis for agricultural research. RAISINS is completely online and doesnot require any downloads. It integrates the power of R, Python, and AI to offer user-friendly, robust statistical tools. The platform is developed by STATOBERRY LLP, with mentorship from the Department of Agricultural Statistics, College of Agriculture, Vellayani, Kerala Agricultural University.\nHead to www.raisins.live where you can access various analytical modules. You can accesss the Cluster Analysis Module from the Analysis module under Analysis of Experiments. See Figure¬†1 to get started with Cluster Analysis in RAISINS\n\n\n\n\n\n\nFigure¬†1: Cluster analysis in RAISINS"
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#how-to-prepare-your-data",
    "href": "tutorial/cluster/Clustertutorial.html#how-to-prepare-your-data",
    "title": "Hierarchical Cluster Analysis",
    "section": "2 How to Prepare Your Data?",
    "text": "2 How to Prepare Your Data?\nProper data preparation is crucial for accurate HCA results in RAISINS. What truly matters is the quality of your data! As the saying goes, ‚Äúgarbage in, garbage out‚Äù - and this holds true for any software.\nTo prepare your dataset for analysis in RAISINS, you have two options:\n\nCreate your dataset in MS Excel, see Section¬†2.1\nBuild your dataset directly within the RAISINS app, see Section¬†2.2\n\n\n2.1 Creating a Data File in MS Excel\nRAISINS allows for analyzing multiple characters simultaneously. So you can enter the observations for all characters for a particular set of treatments in a single excel file.\n\nOpen a new microsoft excel file, use single sheet only.\nStart with Cell A1: Open a new MS Excel file and begin entering data from cell A1. Do not leave any blank rows above.\nFirst Row - Column Names: The first row must contain the column names.\nColumn 1: Enter treatment/labels. There should not be any repetition in the label ID‚Äôs. If there is replications, you can use the mean values.\nFrom Column 2 Onwards: Enter the names of each variable under study as separate columns (e.g.Murder, Assault, UrbanPop, Rape). You can give any names to the columns.\n\nSee Figure¬†2 showing how the prepared Excel file for upload should look like\n\n\n\n\n\n\nFigure¬†2: Model1 showing how the prepared Excel file for upload should look like\n\n\n\n\n2.1.1 File Format Recommendation\n\nIn RAISINS you can upload file in xls, xlsx or csv format. But we recommend a csv format as it will be much lighter.\n\n\n\nFollow the below steps save your Excel file in csv format.\n\n\nOpen your Excel file.\nClick on File in the top-left corner.\nSelect Save As from the menu.\nChoose the location where you want to save the file.\nIn the Save as type dropdown menu, select CSV (Comma delimited) (*.csv).\nEnter a name for your file.\nClick Save.\n\n\n\n2.1.2 Naming Columns and Treatments\n\nKeep It Simple and Straight (KISS): Use simple and short names for column headers.\nAvoid Complexity: Do not include units, special characters, or spaces in names (e.g., use char1 instead of Character 1). This helps ensure the names appear neatly in plots and outputs.\nThis helps ensure the names appear neatly in plots and outputs.\n\n\n\nDataset Creation Rules\n\n\nColumn Naming Convention\n\nNo spaces allowed in column names.\n\nUse underscores (_) or full stops (.) for separation.\nAvoid symbols and special characters like %,# etc\n\nData Arrangement\n\nStart data arrangement towards the upper-left corner.\n\nEnsure the row above the data is not blank.\n\nCell Management\n\nAvoid typing or deleting in cells without data.\n\nIf needed, select affected cells, right-click, and select Clear Contents.\n\nColumn Relevance\n\nName all columns meaningfully.\n\nExclude unnecessary columns not required for analysis.\n\n\n\n\n\n\n2.2 Creating Data Using the App (Recommended Method)\nSee Figure¬†3 and follow the below steps to create data in RAISINS\n\nNavigate to Create Data Tab: Click on the Create Dataset tab in the main menu at the top of the app.\nSpecify Details: Enter the levels of Factor A, Factor B, Factor C and number of characters under study in the window that opens.\nThen click create\nModel Data Entry File: A template for data entry will be generated. You can:\n\nDirectly enter your data into this template.\nOr, copy-paste data from an existing Excel file.\n\nDownload as CSV: Once the data is entered, click on the Download CSV File button. The downloaded CSV file can be uploaded for analysis in Analysis tab.\n\n\n\n\n\n\n\nFigure¬†3: Creating Data in RAISINS"
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#how-to-upload-the-data-file-for-analysis",
    "href": "tutorial/cluster/Clustertutorial.html#how-to-upload-the-data-file-for-analysis",
    "title": "Hierarchical Cluster Analysis",
    "section": "3 How to upload the data file for analysis ?",
    "text": "3 How to upload the data file for analysis ?\n\nUpload the Prepared CSV/XLS File: Go to the Analysis tab(See Figure¬†4) and upload your prepared data file by clicking Browse.\n\n\n\n\n\n\n\nFigure¬†4: Analysis tab"
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#how-to-run-analysis",
    "href": "tutorial/cluster/Clustertutorial.html#how-to-run-analysis",
    "title": "Hierarchical Cluster Analysis",
    "section": "4 How to Run analysis?",
    "text": "4 How to Run analysis?\nRAISINS is designed for a smooth and hassle-free experience. Once you click the Run Analysis button, all relevant results and outputs appear instantly-leaving no room for confusion.\n\n\n\n\n\n\nFigure¬†5: Run Analysis Tab in RAISINS\n\n\n\n\n4.1 Different plot types\nA dendrogram is a tree-like diagram that visually represents how hierarchical clustering groups data step by step, allowing you to explore relationships and cluster formation within a dataset. A dendrogram displays the process of merging (or sometimes splitting) clusters during hierarchical cluster analysis.\n\n\nStep-by-Step: How to read a Dendrogram ?\n\n\nStart at the bottom - Each leaf represents a single observation or sample.\nFollow the lines upward - The first merges connect the most similar items. These connections indicate that the points or small clusters joined are very close based on the chosen similarity measure.\nMerging clusters - As you go higher, clusters are combined with others, and each branching indicates additional clustering - think of it as building a family tree for your data.\nBranch heights - The height at which branches merge tells you how similar the merged groups are-the lower the connection, the more similar the groups.\nDeciding number of clusters - By ‚Äúcutting‚Äù the tree horizontally at a given level (distance threshold), you divide the data into clusters-each branch below the cut line forms a cluster.\n\n\nWe‚Äôve ensured that every possible plot type related to the Hierarchical Cluster analysis is readily available. Color-Coded Clusters dendrogram performed with the complete linkage method and euclidean distance metric, identified through elbow method, resulting in 2 clusters is available by default and each dendrogram comes with a gear icon at the top-left corner, allowing you to customize its appearance. You can also download these plots in high-quality PNG (300 dpi), TIFF or PDF formats for use in reports or presentations.(See Figure¬†5)\nExplore Figure¬†6, Figure¬†7, Figure¬†8, Figure¬†9, Figure¬†10, Figure¬†11, Figure¬†12, Figure¬†13 where each dendrogram available in RAISINS is visually illustrated and accompanied by a clear, insightful description below, helping you easily understand the structure and meaning of each clustering pattern.\n\n\n\n\n\n\n\n\n\nFigure¬†6: Color coded clusters (k = 2)\nA color-coded cluster dendrogram is an enhanced version of the basic dendrogram where branches or clusters are colored differently to clearly distinguish the groupings formed by hierarchical clustering. This makes interpreting the tree much easier, especially for large datasets or when clusters overlap visually.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†7: Vertical Dendrogram (k = 2)\nA vertical dendrogram is a classic, user-friendly way to display hierarchical clustering results with the data points along the bottom and cluster merges represented by upward vertical lines, enabling easy interpretation of cluster hierarchy and distances.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8: Horizontal Dendrogram (k = 2)\nA horizontal dendrogram is a tree-like diagram used to visualize hierarchical clustering results where the structure extends horizontally rather than vertically. In this layout, the individual observations or samples appear along the vertical axis, and the branches stretch left to right, representing how clusters merge step-by-step based on dissimilarity or distance.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9: Ward‚Äôs Linkage (k = 2)\nWard‚Äôs linkage is a hierarchical clustering method focused on minimizing the increase in total within-cluster variance (or error sum of squares) when merging clusters. It is widely used because it tends to produce compact, spherical clusters, making it popular in many applications like biological data, market segmentation, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†10: Rectangular Clusters (k = 2)\nRectangular clusters are simply the visual aids drawn as boxes around groups of points identified as clusters on dendrograms or heat maps to facilitate clear, intuitive interpretation of cluster structure in hierarchical clustering outputs.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11: Circular dendrogram (k = 2)\nA Circular Dendrogram is a radial, space-efficient alternative to the classical dendrogram, arranging hierarchical cluster branches in concentric circles around a center. It enhances visual clarity for large or complex datasets by providing a 360-degree view of similarity and cluster structures, making it a valuable tool for hierarchical data visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12: Colored Rectangular Dendrogram (k = 2)\nA coloured rectangular dendrogram is an enhanced dendrogram visualization where clusters are not only enclosed in rectangular boxes but these boxes and the associated branches are also color-coded to clearly distinguish different clusters. This combination improves interpretability and presentation of hierarchical clustering. results.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13: Classical Dendrogram (k = 2) A classical dendrogram is the traditional tree-like diagram used in hierarchical cluster analysis to represent the nested grouping of objects based on their similarity or distance. It is a fundamental visualization tool that shows how clusters are formed progressively, starting from each individual object and merging step by step into larger clusters until all objects are grouped into one.\n\n\n\n\n\n\n\n\n\n4.2 Additional options available\nIn addition to the various plot types available in RAISINS, the platform also provides several customization options to refine your clustering analysis (See Figure¬†5)\nYou can explore and adjust the following settings:\n\n4.2.0.1 Linkage Methods available in RAISINS - single, complete, average, ward.D2, mcquitty, median and centroid.\n\n\nLets understand the different linkage methods\n\n\n\n\n\n\n\nüîπ Single linkage\n\n\n\n\n\nSingle linkage defines the distance between two clusters as the minimum distance between any pair of points, one from each cluster. This approach often produces elongated, chain-like clusters because it merges clusters based on the closest individual pair of points. It is sensitive to noise and outliers, as a single close pair can link otherwise distant clusters.\n\n\n\n\n\n\n\n\n\nüîπ Complete linkage\n\n\n\n\n\nComplete linkage measures the distance between two clusters as the maximum distance between any point in one cluster and any point in the other cluster. This method tends to produce more compact and spherical clusters by enforcing that all members within a cluster are close to each other. It is less sensitive to outliers compared to single linkage.\n\n\n\n\n\n\n\n\n\nüîπ Average linkage\n\n\n\n\n\nAverage linkage computes the clustering distance as the average of all pairwise distances between members of the two clusters. It balances the tendency of single and complete linkage, typically yielding clusters that are more balanced in shape and size by considering all members evenly.\n\n\n\n\n\n\n\n\n\nüîπ Ward.D2 Linkage\n\n\n\n\n\nWard‚Äôs linkage merges clusters such that the increase in total within-cluster variance is minimized. It uses a variance-minimizing criterion based on squared Euclidean distances and tends to produce compact, spherical clusters. Ward.D2 specifically refers to Ward‚Äôs method with squared distances, popular for its stable and interpretable clusters.\n\n\n\n\n\n\n\n\n\nüîπ McQuitty Linkage\n\n\n\n\n\nThis method uses a simple average of distances weighted by cluster sizes when merging clusters. It‚Äôs a variant of average linkage and provides a compromise that accounts for different cluster sizes during the merge.\n\n\n\n\n\n\n\n\n\nüîπ Median Linkage\n\n\n\n\n\nMedian linkage calculates the distance between clusters based on the median of the points within each cluster (cluster median) rather than the mean. This method can be more robust to skewed data distributions but may result in non-monotonic clustering (reversals) in dendrograms.\n\n\n\n\n\n\n\n\n\nüîπ Centroid Linkage\n\n\n\n\n\nCentroid linkage computes the distance between two clusters as the Euclidean distance between their centroids (mean vectors). It effectively merges clusters based on the center of mass but may produce inversions (discontinuities) in the dendrogram if the merging decreases cluster distances temporarily..\n\n\n\n\n\n\n4.2.0.2 Distance Metrics available in RAISINS: Euclidean, Manhattan, Maximum, Canberra, Minkowski.\n\n\nLets understand the different Distance Metrics\n\n\n\n\n\n\n\nüîπ Euclidean Distance\n\n\n\n\n\nEuclidean distance is the most familiar form of distance measurement, representing the straight-line distance between two points in Euclidean space. It is calculated using the Pythagorean theorem, where the distance between two points is the square root of the sum of the squared differences across all dimensions. For example, in a two-dimensional space, the distance between points (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ) is given by the formula ‚àö((x‚ÇÇ ‚àí x‚ÇÅ)¬≤ + (y‚ÇÇ ‚àí y‚ÇÅ)¬≤). This metric measures the shortest path between points, like stretching a string tight between two points on a map, making it widely used in machine learning, such as clustering and classification algorithms.\n\n\n\n\n\n\n\n\n\nüîπ Manhattan Distance\n\n\n\n\n\nManhattan distance, also known as city block distance, sums the absolute differences of the coordinates between two points in a grid-like path. Unlike Euclidean distance, which measures the shortest ‚Äúas-the-crow-flies‚Äù route, Manhattan distance measures how far apart two points are if you can only move along grid lines‚Äîlike navigating city streets laid out in a grid pattern. For example, the distance between (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ) is |x‚ÇÇ ‚àí x‚ÇÅ| + |y‚ÇÇ ‚àí y‚ÇÅ|, often used in urban planning and in applications where movement is restricted to axes-aligned paths.\n\n\n\n\n\n\n\n\n\nüîπ Maximum Distance\n\n\n\n\n\nMaximum distance, also known as Chebyshev distance, considers the greatest absolute difference among all coordinate pairs between two points. It effectively measures how far apart two points are in terms of the most significant coordinate difference. Mathematically, for points (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ), it is max(|x‚ÇÇ ‚àí x‚ÇÅ|, |y‚ÇÇ ‚àí y‚ÇÅ|). This metric is useful in chess (king‚Äôs move), robotics (max step size), and in certain clustering methods, where the largest coordinate gap dominates the distance calculation.\n\n\n\n\n\n\n\n\n\nüîπ Canberra Distance\n\n\n\n\n\nCanberra distance is a weighted version of difference measurement that emphasizes smaller differences, especially when values are close to zero. It is calculated as the sum of the ratios |x‚ÇÇ ‚àí x‚ÇÅ| / (|x‚ÇÇ| + |x‚ÇÅ|) across all dimensions. Because it gives more weight to differences when values are small, it is effective for datasets with many small or sparse values, often used in bioinformatics, economics, and fields dealing with relative differences.\n\n\n\n\n\n\n\n\n\nüîπ Minkowski Distance\n\n\n\n\n\nMinkowski distance is a generalization of Euclidean and Manhattan distances, characterized by a parameter p, called the order. When p=1, Minkowski becomes Manhattan distance; when p=2, it becomes Euclidean distance. For other values of p, it defines a different form of distance calculation, where higher p emphasizes larger differences more heavily, and lower p emphasizes smaller differences. The formula involves taking the p-th root of the sum of the absolute differences raised to the power p across all dimensions, making it very flexible for various applications.\n\n\n\n\n\n\n\n\n\nüîπ Median and Centroid\n\n\n\n\n\nMedian distance refers to the measure of the difference between data points based on their median values; it is more robust to outliers in skewed data distributions. Centroid distance specifically considers the Euclidean distance between the centroids (mean points) of data clusters. The centroid-based method is widely used in clustering techniques like k-means because it measures the central point of a cluster, making it useful for cluster center-based algorithms but sensitive to outliers, which can shift the centroid.\n\n\n\n\n\n\n4.2.0.3 Optimal k Methods available in RAISINS : Elbow, Silhouette, Gap Statistic.\n\n\nLets understand the different optimal k Methods\n\n\n\n\n\n\n\nüîπ Elbow Method\n\n\n\n\n\nThe elbow method is a heuristic used in clustering analysis, particularly with K-means, to determine the optimal number of clusters. It involves plotting the within-cluster sum of squares (WCSS) against different values of k (number of clusters). As the number of clusters increases, WCSS decreases because the data points within each cluster become more tightly grouped. The goal is to identify the ‚Äúelbow‚Äù point in the plot where the rate of decrease sharply changes, indicating an optimal balance between minimizing intra-cluster variance and avoiding overfitting. This point suggests the most appropriate number of clusters, where adding more clusters yields diminishing improvements.\n\n\n\n\n\n\n\n\n\nüîπ Silhouette Method\n\n\n\n\n\nThe Silhouette method measures how similar an object is to its own cluster compared to other clusters. For each data point, the Silhouette score ranges from -1 to 1, with higher scores indicating better clustering (points are closer to members of their own cluster than to other clusters). The average silhouette score across all points for different cluster numbers helps identify the optimal k: the higher the average score, the better the clustering. This method considers both cohesion within clusters and separation between clusters, providing a more nuanced evaluation of clustering quality than solely looking at variance.\n\n\n\n\n\n\n\n\n\nüîπ Gap Statistic\n\n\n\n\n\nThe Gap Statistic compares the total within-cluster dispersion for different k values with their expected dispersion under a null reference distribution (usually a uniform distribution of points). It calculates the gap between the observed clustering and the expected clustering under randomness: a larger gap indicates more meaningful, well-separated clusters. The optimal number of clusters is the one where the gap statistic reaches its maximum, suggesting that the clustering structure differs significantly from random noise. This method is robust but more computationally intensive, as it involves generating multiple reference datasets for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14: Elbow plot\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†15: Silhouette plot\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†16: Gap Statistic\n\n\n\n\n\n\n\n\n4.2.0.4 Scaling Options available in raisins: zscore, center, minmax, unitlength, robust, none.\n\n\nLets understand the different scaling options\n\n\n\n\n\n\n\nüîπ zscore\n\n\n\n\n\nZ-score normalization, also known as standardization, transforms data so that it has a mean of 0 and a standard deviation of 1. This method computes the Z-score for each data point by subtracting the dataset‚Äôs mean (Œº) and dividing by its standard deviation (œÉ). The resulting scaled data indicates how many standard deviations each point is from the mean. It is particularly useful for datasets with roughly normal distributions and algorithms that assume normality, such as linear regression or SVM, as it removes scale bias and enhances comparability across features.\n\n\n\n\n\n\n\n\n\nüîπ Center\n\n\n\n\n\nCentering data refers to subtracting the mean of a feature from each data point, effectively shifting the data so the mean becomes zero without changing the data‚Äôs variance or distribution shape. This operation is fundamental to many data preprocessing steps, such as Z-score normalization, and helps algorithms interpret features more straightforwardly by aligning data around a common point, making the analysis less affected by location shifts.\n\n\n\n\n\n\n\n\n\nüîπ MinMax\n\n\n\n\n\nMin-Max scaling rescales data to a fixed range, typically. It adjusts each feature by subtracting the minimum value and dividing by the range (max - min), effectively compressing all data within the specified bounds. This technique preserves the original distribution but is highly sensitive to outliers, which can distort the scaled values by expanding the range. Min-Max normalization is often used in neural networks and gradient-based algorithms where feature bounds matter.\n\n\n\n\n\n\n\n\n\nüîπ Unit Length\n\n\n\n\n\nUnit length scaling normalizes each data sample (or feature vector) so that its total length (or Euclidean norm) equals 1. This is achieved by dividing each vector by its magnitude, which is the square root of the sum of squared components. It‚Äôs commonly used in text analysis (e.g., TF-IDF vectors) and machine learning algorithms that rely on cosine similarity, as it ensures all vectors are on the same scale, emphasizing direction over magnitude.\n\n\n\n\n\n\n\n\n\nüîπ Robust\n\n\n\n\n\nRobust scaling uses statistics less sensitive to outliers, typically by subtracting the median and dividing by the interquartile range (IQR). This method centers the data around the median and scales it according to the spread of the middle 50% of data points, making it effective when dealing with noisy data or datasets with extreme outliers. Its primary goal is to produce scaled data that reflects the true distribution of the bulk of the data without being skewed by outliers.\n\n\n\n\n\n\n4.2.0.5 Number of Clusters: Specify or adjust the number of clusters to be formed based on your analysis needs.\n\n\n\n4.3 Heatmap in RAISINS\nA heatmap in cluster analysis is a visualization technique that represents data values in a matrix format using colors to indicate magnitude, typically after both the rows and columns have been reordered based on hierarchical clustering results. The integration of heatmaps with cluster analysis allows you to visually explore and identify patterns, groupings, or similarities among observations (rows) and variables (columns).\nIn the RAISINS platform after uploading your data click on the Heatmap tab to generate the heatmap and you can use the gear icon to customize your heatmap (See Figure¬†17) By default the data are standardized using the zscore scaling method, ensuring all features contribute equally to clustering, which was performed using the complete linkage method and euclidean distance metric, resulting in 2 clusters.You can also download the heatmapin high-quality PNG (300 dpi), TIFF or PDF formats for use in reports or presentations.\n\n\n\n\n\n\nFigure¬†17: How to access the Heatmap tab in RAISINS\n\n\n\nSee Figure¬†18 where blocks of similar colors indicate labels or variables with related profiles, and the accompanying dendrograms illustrate their hierarchical relationships, making it easier to identify groups that share similar characteristics or expression patterns.\n\n\n\n\n\n\nFigure¬†18: Heatmap in RAISINS\n\n\n\n\n\n4.4 The four important statistical components available in RAISINS\nIn the RAISINS platform after uploading your data select the labels then select the clustering variables of your choice, then click on Run Analysis and go to Metrics (See Figure¬†19) to generate Distance matrix, Cluster-wise means, Intra-cluster Statistics and Inter-cluster Statistics tables and you can either copy this file or download these tables in .xlsx or .csv formats for use in reports or presentations.\n\n\n\n\n\n\nFigure¬†19: Metrics tab in RAISINS\n\n\n\nBelow given is a brief description on the four statistical components available in RAISINS\n\nDistance Matrix (euclidean method) - See Figure¬†20\n\n\n\n\n\n\n\nFigure¬†20: Distance Matrix in RAISINS\n\n\n\nIn hierarchical clustering, the distance matrix represents the pairwise dissimilarities or distances between all observations in the dataset. It quantifies how similar or different each observation is from every other observation and serves as the foundation for grouping observations into clusters. The distance matrix provides a reference for understanding the relative closeness of observations and guides the formation of clusters based on similarity.\n\nCluster means - See Figure¬†21\n\n\n\n\n\n\n\nFigure¬†21: Cluster wise means in RAISINS\n\n\n\nIn hierarchical clustering, the cluster mean represents the average values of all variables for the observations within a cluster, summarizing the central tendency of the cluster and providing a reference point to understand the characteristics of the grouped data. It represents the typical characteristics of that cluster.\n\nIntra-cluster Statistics - See Figure¬†22\n\n\n\n\n\n\n\nFigure¬†22: Intracluster statistics\n\n\n\nIn hierarchical clustering, intra-cluster statistics measure the compactness or similarity of observations within the same cluster. They quantify how closely the members of a cluster resemble each other, often using metrics like the average distance between observations and the cluster mean. These statistics help assess the cohesion of a cluster, indicating how homogeneous or tightly grouped the cluster members are.\n\nInter-cluster Statistics - See Figure¬†23\n\n\n\n\n\n\n\nFigure¬†23: Intercluster statistics\n\n\n\nIn hierarchical clustering, inter-cluster statistics measure the separation or dissimilarity between different clusters. They quantify how distinct or far apart the clusters are from each other, often using metrics like the distance between cluster means or centroids. These statistics help assess the distinctness of clusters, indicating how well the clustering algorithm has separated different groups in the dataset.\n\n\n4.5 HCPC (Hierarchical Clustering on Principal Components)\nHierarchical clustering on principal components (HCPC) is a hybrid approach that combines the strengths of principal component analysis (PCA) and hierarchical clustering. PCA first reduces the dimensionality of the dataset by summarizing correlated variables into a smaller set of uncorrelated components that retain most of the original variance. Then, hierarchical clustering is performed on these principal components instead of the raw variables, which enhances the separation of clusters and reduces noise caused by variable correlations. This method is particularly useful for visualizing and identifying meaningful groupings in complex multivariate datasets, as it simplifies interpretation while preserving the main structure of the data.\n\n\nWhen to use HCPC ?\n\n\nWhen you have a large dataset with many continuous variables and want to reduce dimensionality before clustering to improve interpretability and cluster quality.\nWhen your dataset contains categorical variables, multiple correspondence analysis (MCA) or factor analysis can transform them into continuous principal components suitable for clustering.\nWhen original variables are correlated or noisy, HCPC clusters on principal components that summarize the main variance structure and filter noise, leading to more meaningful and stable clusters.\nWhen you want an integrated approach combining principal component analysis for dimensionality reduction, hierarchical clustering for initial cluster identification, and k-means clustering for refinement.\nWhen exploring multivariate data for identifying natural groupings, patterns, or clusters with clearer separation and visual representation (e.g., factor maps, chord diagrams).\nWhen the goal is simplifying complex multivariate data for easier interpretation and presentation in reports or scientific communication.\n\n\n\n\nDifference between HCPC and classical HCA\n\n\nHCPC applies Principal Component Analysis (PCA) first to reduce data dimensionality before clustering; HCA clusters directly on the original variables.\nHCPC clusters the uncorrelated principal components that represent the main variance structure, filtering noise and redundancy; HCA uses raw data and can be influenced by correlated variables.\nHCPC integrates PCA, hierarchical clustering, and k-means clustering to improve cluster stability and quality; HCA only involves hierarchical clustering.\nHCPC is designed for complex, high-dimensional, or mixed-type datasets; HCA is suitable for simpler, low-dimensional data.\nHCPC produces enhanced visualizations like factor maps and chord diagrams for better cluster interpretation; HCA mainly produces dendrograms based on distances.\nHCPC requires additional computational steps but offers more robust and interpretable clusters; HCA is simpler and faster but may lead to less meaningful clusters in complex data.\n\n\nIn the RAISINS platform after uploading your data select the labels then select the clustering variables of your choice, then click on Run Analysis and go to HCPC to generate Factor map, Chord diagram and Chord diagram - Cluster means which are downloadable in PNG, TIFF or PDF formats with interpretation and tables. The tables can be either copied or downloaded in .xlsx or .csv formats for use in reports or presentations.\n\n\n\n\n\n\n\n\n\nFigure¬†24: Factor Map\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†25: Chord diagram\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†26: Chord diagram - Cluster means\n\n\n\n\n\n\nIn the Figure¬†24 the Factor map displays the results of Hierarchical Clustering on Principal Components (HCPC), where observations are grouped into distinct clusters based on their similarity across all variables. The plot is constructed in a reduced two-dimensional space using the first two principal components (Dim1 and Dim2), which capture the maximum variance in the data while preserving the essential structure of the original multidimensional dataset. Each cluster is represented by a different color and enclosed in a convex hull (shaded polygon), making it easy to visually distinguish between groups. Observations within the same cluster share similar characteristics across the measured variables, while observations in different clusters exhibit distinct patterns or profiles. The percentages shown on each axis indicate how much of the total variance in the data is explained by that dimension - higher percentages mean that dimension captures more information about the differences between observations. The spatial separation between clusters reflects how different they are: clusters that are far apart have very different profiles, while clusters that are closer together are more similar. Overlapping regions suggest some ambiguity in cluster boundaries, where observations share characteristics with multiple groups. The position of each observation (data point label) within the plot is determined by its scores on the principal components, allowing you to see not only which cluster each observation belongs to but also how it relates to other observations within and across clusters. This visualization helps identify natural groupings in your data, understand the relationships between observations, and assess the quality of the clustering by examining how well-separated and cohesive the clusters appear in the reduced dimensional space\n\n\n\n\n\n\nFigure¬†27: Top variables\n\n\n\nIn Figure¬†27, the table shows the top contributing variables for each cluster based on the v.test statistic from the HCPC analysis. For each cluster, the five variables with the highest absolute v.test values are displayed. ‚ÄòMean in category‚Äô and ‚Äòsd in category‚Äô show the standardized mean and standard deviation within the cluster, while ‚ÄòOverall mean‚Äô and ‚ÄòOverall sd‚Äô provide reference values across all data. The p.value indicates the statistical significance of the variable‚Äôs contribution. Variables with higher absolute v.test values and lower p.values are more important in defining the cluster.\nIn Figure¬†25, the chord diagram visualizes the relationships between your original variables and the identified clusters from Hierarchical Clustering on Principal Components (HCPC). The outer ring shows two types of segments: your original dataset variables and the clusters (groups of similar observations). The ribbons connecting them represent the strength of association, with wider ribbons indicating that a variable is more characteristic or defining for that particular cluster. The width is determined by v.test statistics, which measure how strongly a variable distinguishes each cluster from others. By examining these connections, you can understand the profile of each cluster - which variables are most prominent in defining each group. Variables with thick ribbons to a cluster are key features of that group, while variables connected to multiple clusters play important roles across different groups. The diagram displays the most characteristic variables for each cluster with weights normalized within each cluster, allowing you to compare the relative importance of variables in defining cluster identities and understand what makes each group unique in your dataset.\n\n\n\n\n\n\nFigure¬†28: PCA Summary Table\n\n\n\nIn Figure¬†28, the table summarizes the results of Principal Component Analysis (PCA), showing how much variance each principal component explains in the dataset. The eigenvalue indicates the amount of variance captured by each component, with larger values representing more important components. The percent column shows the proportion of total variance explained by each component, while the cumulative percent shows the running total across components. Components with eigenvalues greater than 1 are typically considered meaningful. This information helps determine how many components are needed to adequately represent the original data and forms the basis for subsequent clustering analysis.\n\n\n\n\n\n\nFigure¬†29: Cluster quality indicators\n\n\n\nIn Figure¬†29, the table summarizes the variance in the dataset with respect to clustering. Total Sum of Squares (Total_SS) represents the overall variability in the data. Within-Cluster Sum of Squares (Within_SS) measures variability of observations inside each cluster. Between-Cluster Sum of Squares (Between_SS) quantifies variability between cluster centers. The Between/Total Ratio indicates the proportion of total variance explained by the differences between clusters. Higher ratios mean better separation between clusters."
  },
  {
    "objectID": "policy.html",
    "href": "policy.html",
    "title": "RAISINS",
    "section": "",
    "text": "At RAISINS by Statoberry LLP, we are committed to protecting your privacy. We do not collect or store any personally identifiable information unless explicitly provided by the user for communication or support purposes.\n\nWe do not use third-party advertising tools.\nWe do not share your data with any third parties.\nWe collect only anonymized usage data (e.g., feature usage frequency) to improve platform performance and user experience.\n\nYour use of RAISINS is secure, and we ensure that no unauthorized access occurs to any part of your interaction with the platform."
  },
  {
    "objectID": "policy.html#privacy-policy",
    "href": "policy.html#privacy-policy",
    "title": "RAISINS",
    "section": "",
    "text": "At RAISINS by Statoberry LLP, we are committed to protecting your privacy. We do not collect or store any personally identifiable information unless explicitly provided by the user for communication or support purposes.\n\nWe do not use third-party advertising tools.\nWe do not share your data with any third parties.\nWe collect only anonymized usage data (e.g., feature usage frequency) to improve platform performance and user experience.\n\nYour use of RAISINS is secure, and we ensure that no unauthorized access occurs to any part of your interaction with the platform."
  },
  {
    "objectID": "policy.html#data-policy",
    "href": "policy.html#data-policy",
    "title": "RAISINS",
    "section": "Data Policy",
    "text": "Data Policy\nRAISINS operates on a temporary, cloud-based computing environment.\n\nNo uploaded data is stored permanently.\nEvery analysis session is a virtual instance. Once the session ends (i.e., when the user closes the app or navigates away), all uploaded files, temporary outputs, and analysis results are permanently deleted.\nNo backup copies are created, and no user input is saved unless explicitly exported by the user.\n\nThis ensures that your data remains confidential and fully under your control. We do not retain any files, results, or metadata beyond the duration of your session.\n\nüí° In short: No uploaded data is stored anywhere. Once the user quits, everything in that session is lost forever."
  },
  {
    "objectID": "policy.html#refund-policy",
    "href": "policy.html#refund-policy",
    "title": "RAISINS",
    "section": "Refund Policy",
    "text": "Refund Policy\nStatoberry LLP strives to provide a high-quality experience with RAISINS. Our refund policy is as follows:\n\nFor one-time purchases or subscriptions made in error, please contact us within 7 days of payment.\nRefunds are not issued if a module was purchased in error; however, we will gladly switch access to the correct module upon request.\nRefunds are only considered if the user was unable to access a module for more than one day (24 hrs) due to technical failure.\nInstitutional and multi-user licenses are non-refundable once activated, except in cases of verified technical failure that prevents access.\n\nTo request a refund, email us at support@statoberry.com with your order details and reason for the request. We review all requests individually and aim to process valid claims within 7 working days.\n\nFor questions about any of the above policies, feel free to contact us at support@statoberry.com."
  },
  {
    "objectID": "issue.html",
    "href": "issue.html",
    "title": "RAISINS",
    "section": "",
    "text": "Thank you for helping improve RAISINS. If you‚Äôve encountered a bug, suggestion, or problem, we provide two simple ways to reach us:\n\n\n\nIf you have a GitHub account, click below to file an issue directly in our GitHub repository:\n  File a GitHub Issue \n\n\n\n\nIf you do not have a GitHub account: File a feedback‚Üí\nyou can email us:\n\nüì¨ Email: support@statoberry.com\nüìû Contact page: Go to contact‚Üí\n\n\n\n\n\nWe do not store any uploaded data or personal information. Every session is temporary and deleted once you close the app.\n\nFor more, read our Privacy & Data Policy.\n\n\nIf you‚Äôre reporting a technical issue, please include: - The name of the module - A brief description of the issue - A screenshot or error message (if any)"
  },
  {
    "objectID": "issue.html#report-an-issue",
    "href": "issue.html#report-an-issue",
    "title": "RAISINS",
    "section": "",
    "text": "Thank you for helping improve RAISINS. If you‚Äôve encountered a bug, suggestion, or problem, we provide two simple ways to reach us:\n\n\n\nIf you have a GitHub account, click below to file an issue directly in our GitHub repository:\n  File a GitHub Issue \n\n\n\n\nIf you do not have a GitHub account: File a feedback‚Üí\nyou can email us:\n\nüì¨ Email: support@statoberry.com\nüìû Contact page: Go to contact‚Üí\n\n\n\n\n\nWe do not store any uploaded data or personal information. Every session is temporary and deleted once you close the app.\n\nFor more, read our Privacy & Data Policy.\n\n\nIf you‚Äôre reporting a technical issue, please include: - The name of the module - A brief description of the issue - A screenshot or error message (if any)"
  },
  {
    "objectID": "discuss.html",
    "href": "discuss.html",
    "title": "RAISINS",
    "section": "",
    "text": "Welcome to the Discussion Corner!\nThis is an open space for researchers, students, and data enthusiasts to discuss anything related to statistics, data analysis, and research methodology, not just limited to RAISINS. Have a new idea for a statistical method? Struggling with the right technique for your experiment? Recently published a paper or came across an interesting approach in your field? Let‚Äôs explore it together.\nWe‚Äôre listening and evolving. If there‚Äôs a feature, model, or analysis you think would benefit others, suggest it here. Your contributions can directly shape what RAISINS offers next.\nLet‚Äôs build a vibrant and collaborative statistical community"
  },
  {
    "objectID": "discuss.html#discussion_corner",
    "href": "discuss.html#discussion_corner",
    "title": "RAISINS",
    "section": "",
    "text": "Welcome to the Discussion Corner!\nThis is an open space for researchers, students, and data enthusiasts to discuss anything related to statistics, data analysis, and research methodology, not just limited to RAISINS. Have a new idea for a statistical method? Struggling with the right technique for your experiment? Recently published a paper or came across an interesting approach in your field? Let‚Äôs explore it together.\nWe‚Äôre listening and evolving. If there‚Äôs a feature, model, or analysis you think would benefit others, suggest it here. Your contributions can directly shape what RAISINS offers next.\nLet‚Äôs build a vibrant and collaborative statistical community"
  },
  {
    "objectID": "discuss.html#feedback",
    "href": "discuss.html#feedback",
    "title": "RAISINS",
    "section": "Feedback",
    "text": "Feedback\nYour suggestions, bug reports, and ideas are incredibly valuable to us. Whether you‚Äôre facing an issue, have a feature request, or just want to share your thoughts this is the place to do it.\nLoading‚Ä¶"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "RAISINS",
    "section": "",
    "text": "üì¨ Get in touch with us\n\n  \n    \n    \n      \n        \n          General Support\n          \n            Email: support@statoberry.com\n            Doubts: statistician@statoberry.com\n          \n        \n      \n    \n    \n    \n      \n        \n          Phone & WhatsApp\n          \n            Contact: Hisham M., +91 99475 99837\n            Alternate: +91 97469 19099, +91 62382 95604\n            WhatsApp: +91 95268 50087\n          \n        \n      \n    \n    \n    \n      \n        \n          Mentor\n          \n            Dr. Pratheesh P Gopinath\n            Asst. Professor, Department of Agricultural Statistics\n            College of Agriculture, Vellayani\n            Kerala Agricultural University\n            Email: pratheesh.pg@kau.in"
  },
  {
    "objectID": "citation.html",
    "href": "citation.html",
    "title": "1 Cite Us ‚Äì RAISINS",
    "section": "",
    "text": "1 Cite Us ‚Äì RAISINS\n\n\n\n  \n  \n  \n\n \n\n  If you are using RAISINS, please cite the following publication:\n\n  \n  \n    APA Style\n    Hisham M, M., Chandran, J., & Gopinath, P. P. (2025). RAISINS: Integrating R and AI for Agricultural Data Analysis. Journal of Sustainable Technology in Agriculture, 1(1). PAPAYA Academic Press. https://doi.org/10.5281/zenodo.15622128\n  \n\n  \n  \n    Harvard Style\n    Hisham M, M., Chandran, J. and Gopinath, P.P., 2025. RAISINS: Integrating R and AI for Agricultural Data Analysis. Journal of Sustainable Technology in Agriculture, 1(1). PAPAYA Academic Press. Available at: https://doi.org/10.5281/zenodo.15622128\n  \n\n  \n  \n    BibTeX\n    @article{mohammed2025raisins,\n  title     = {RAISINS: Integrating R and AI for Agricultural Data Analysis},\n  author    = {Hisham, M. M. and Chandran, J. and Gopinath, P. P.},\n  journal   = {Journal of Sustainable Technology in Agriculture},\n  volume    = {1},\n  number    = {1},\n  year      = {2025},\n  publisher = {PAPAYA Academic Press},\n  doi       = {10.5281/zenodo.15622128},\n  url       = {https://doi.org/10.5281/zenodo.15622128}\n}\n  \n\n  \n  \n    üîó View on Zenodo\n    üìÑ View Online Version"
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "Welcome to RAISINS",
    "section": "Analysis Modules",
    "text": "Analysis Modules\nDoing a statistical analysis has never been easier. Simply select the appropriate analysis module listed below to launch the application. Our platform operates on secure cloud infrastructure to ensure high-performance computing for your data analysis. To support sustained access and development, a modest subscription is required per module. These plans are thoughtfully structured to remain affordable for students and researchers. Once subscribed, access the app and upload your dataset and click Run Analysis the results are generated instantly, with accuracy and clarity.\n\n\n\n\n\n\n Preview Mode\n\n\n\nExplore any app for free using  Preview Mode ‚Äî no registration needed. Just enter your email and OTP to begin. Use this mode for Explore, teaching, training programmes or hands-on practice. To upload your own data for analysis, a subscription is required. Model datasets are available within Preview Mode.\nMaking Statistics Sweet\n\n\n\nData Analysis\n\n\n\n\n  \n\n    \n    \n      \n         Descriptive Statistics\n      \n\n      \n      \n        \n      \n            \n      \n        \n      \n      \n      \n    \n    \n      \n         Two Sample t-test\n      \n\n      \n      \n        \n      \n\n      \n      \n        \n      \n    \n\n    \n    \n      \n         Regression Analysis\n      \n\n      \n        \n      \n\n      \n        \n      \n    \n\n    \n    \n      \n         Cluster Analysis\n      \n\n      \n        \n      \n\n      \n        \n      \n\n      \n        \n      \n    \n    \n  \n    \n      Principal Component Analysis (PCA)\n      \n      \n      \n      \n      \n           \n        \n      \n      \n       \n\n  \n\n\n\n\n\n\nAnalysis of Experiments\n\n\n\n\n  \n\n    \n    \n      Single Factor Experiments\n    \n    \n      \n         CRD (Completely Randomized Design)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n      \n         RBD (Randomized Block Design)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n      \n         ANCOVA in CRD\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n     \n      \n         ANCOVA in RBD\n      \n      \n            \n      \n        \n      \n      \n        \n      \n      \n\n    \n    \n      \n         Repeated Measures ANOVA (Two way)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n    \n      Two Factor Experiments\n    \n    \n      \n         2FCRD (Two Factor Factorial CRD)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n      \n         2FRCBD (Two Factor Factorial RBD)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n      \n         Split-plot Designs\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n      \n         Strip-plot Designs\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n\n    \n    \n      Three Factor Experiments\n    \n    \n      \n         3FCRD (Three Factor Factorial CRD) \n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n      \n         3FRBD (Three Factor Factorial RBD)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n      \n         Split-Split Plot Analysis\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n    \n      Pooled Analysis\n    \n    \n    \n      \n         CRD Pooled Analysis \n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n      \n      \n    \n      \n         RBD Pooled Analysis\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n    \n      \n         Two factor CRD Pooled Analysis  \n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n      \n    \n    \n      \n         Two factor RBD Pooled Analysis \n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n\n    \n    \n      \n         Pooled Split-plot analysis\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n    \n      \n         Pooled Strip-plot analysis\n      \n            \n      \n        \n      \n      \n        \n      \n    \n    \n    \n    \n      Non Parametric\n    \n    \n      \n         Kruskal-Wallis Test\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n  \n\n\n\n\n\n\nStatistical Genetics\n\n\n\n\n  \n  \n    \n      \n         Stability Analysis (Eberhart and Russell's Model)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n        \n      \n         AMMI Analysis (RBD) \n      \n      \n      \n        \n      \n      \n            \n      \n        \n      \n      \n          \n    \n    \n      \n         Path Analysis (RBD)\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n      \n         Compact Family Block Design\n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n       \n      \n         Diallel Analysis (Hayman's Approach) \n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n    \n    \n     \n    \n      Line X Tester Analyis\n      \n      \n      \n      \n      \n           \n        \n      \n      \n      \n          \n     \n    \n      Variance Component Analyis New Release: 22/12/2025 \n      \n      \n      \n      \n      \n      \n\n  \n\n\n\n\n\n\nSocial Science Tools\n\n\n\n\n  \n  \n    \n      \n         Sample Size Calculator\n      \n      \n         Free\n      \n    \n\n    \n      \n         Exploratory Factor Analysis\n      \n      \n        \n      \n      \n        \n      \n    \n\n    \n      \n         Binary Logistic Regression\n      \n      \n        \n      \n      \n        \n      \n    \n\n    \n      \n         Reliability and Validity\n      \n      \n        \n      \n      \n        \n      \n    \n  \n\n\n\n\n\nCustom Tools\n\nTools for KAU\n\n\n\n\n  \n  \n    \n      \n         CABBAGE for KAU referencing style\n      \n      \n      \n        \n      \n    \n  \n\n\n\n\n\n\n\n\n\nInstitutional login\n\n\n\nAt RAISINS, we deeply value academic collaborations and are committed to making statistical analysis easy, affordable, and enjoyable. If you believe your institution could benefit from access to RAISINS, we encourage you to inform the relevant authority to get in touch with us. We offer customised plans, our unique flavours‚Äîto match the research needs of your institution.Click here to explore Institutional Licensing for RAISINS.\n\n\n\n\n  üèõÔ∏è institutions with access\n\n  \n    \n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n    \n    \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n    \n      0\n      Users\n    \n  \n\n  \n    \n    \n      0\n      Institutions\n    \n  \n\n  \n    \n    \n      0\n      User Hours\n    \n  \n\n\n\n\n\n\n  \n  Learn more ‚Üí"
  },
  {
    "objectID": "index.html#what-our-users-say",
    "href": "index.html#what-our-users-say",
    "title": "Welcome to RAISINS",
    "section": "üí¨ What Our Users Say",
    "text": "üí¨ What Our Users Say\n\n  \n    \n\n      \n      \n        \n          \n            Peram Nagaseshi Reddy, Ph.D. Scholar\n            \n            ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n            As an agricultural student, I initially found statistics challenging. Thanks to RAISINS, I published my M.Sc. manuscript and now use it in my Ph.D. research. Highly recommended!\n          \n        \n      \n\n      \n      \n        \n          \n            ABHILA S R, Assistant Professor, KAU\n            \n            ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n            This site makes statistics so much easier to understand! Clean interface, beginner-friendly tools‚Äîperfect for students and quick analysis. üòä\n          \n        \n      \n\n      \n      \n        \n          \n            Dr Priya Kumari I, Asst Prof. Floriculture, KAU\n            \n            ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n            Started using RAISINS. Simple and super. All the analysis in seconds. Graphs, plots, interpretation‚Äîall in a single click. Great work!\n          \n        \n      \n\n      \n      \n        \n          \n            Neha E S, PhD Scholar, Food and Nutrition\n            \n            ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n            RAISINS is a miracle in digital form! What once took me hours now takes minutes with clean, publication-ready tables. Eternal gratitude to the creators!\n          \n        \n      \n\n      \n      \n        \n          \n            Arindam Deb\n            \n            ‚≠ê‚≠ê‚≠ê‚≠ê\n            I've been using RAISINS extensively. The ready-made tables are a life saver. A few tweaks like axis and color control would be great additions!\n          \n        \n      \n      \n      \n\n  \n    \n      Shankarprasad K S, KAU Alumni\n      \n      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n      \n        The most efficient and smoothly operating statistical tool for students and researchers for all the basic analysis.\n      \n    \n  \n\n\n\n\n  \n    \n      Manjeet Singh, Research Scholar, ICRISAT\n      \n      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n      \n        The tool is awesome and very user friendly, can easily do data analysis with huge datasets with high precision.\n        The software gives detailed analysis. Highly recommended to try.\n      \n    \n  \n\n\n    \n\n    \n    \n      \n        ‚Üê Prev\n      \n      \n        Next ‚Üí\n      \n    \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n\n  \n\n\n\n\n\n  \n    \n      \n        \n           Subscription Plan"
  },
  {
    "objectID": "plans.html",
    "href": "plans.html",
    "title": "RAISINS",
    "section": "",
    "text": "RAISINS is built by researchers, for researchers - a platform developed by a team of PhD scholars in Statistics who understand the real needs of the academic community.\nOur plans are thoughtfully designed to support students and early-career researchers in accessing quality statistical tools without financial burden.\nTo make it simple and flexible, each RAISINS app functions as an independent module.\n\n\n\nYou can subscribe only to the tools you need, starting at just ‚Çπ199 per month and there‚Äôs no requirement to renew if you don‚Äôt need it in the next month.\n\n\nCurrently, no usage limits are enforced. If you ever reach one (rarely), just drop us an email we‚Äôll unlock it for you.\n\n\nSimply click on any app listed under Analysis Modules and choose a monthly or 6-month license.\nThis modular subscription is perfect for students and researchers who need specific tools for a limited period.\n\n\n\nBefore subscribing, you can explore all RAISINS modules in preview mode using our built-in sample datasets. It‚Äôs the best way to experience RAISINS before committing.\n\n\nView Tutorial ‚Üí\n\n\n\nModule licenses give access to individual tools which is ideal for students or personal users focused on a single analysis. If you‚Äôre a scientist or assistant professor involved in active research, explore our Individual Plan for advanced access."
  },
  {
    "objectID": "plans.html#module-license",
    "href": "plans.html#module-license",
    "title": "RAISINS",
    "section": "",
    "text": "RAISINS is built by researchers, for researchers - a platform developed by a team of PhD scholars in Statistics who understand the real needs of the academic community.\nOur plans are thoughtfully designed to support students and early-career researchers in accessing quality statistical tools without financial burden.\nTo make it simple and flexible, each RAISINS app functions as an independent module.\n\n\n\nYou can subscribe only to the tools you need, starting at just ‚Çπ199 per month and there‚Äôs no requirement to renew if you don‚Äôt need it in the next month.\n\n\nCurrently, no usage limits are enforced. If you ever reach one (rarely), just drop us an email we‚Äôll unlock it for you.\n\n\nSimply click on any app listed under Analysis Modules and choose a monthly or 6-month license.\nThis modular subscription is perfect for students and researchers who need specific tools for a limited period.\n\n\n\nBefore subscribing, you can explore all RAISINS modules in preview mode using our built-in sample datasets. It‚Äôs the best way to experience RAISINS before committing.\n\n\nView Tutorial ‚Üí\n\n\n\nModule licenses give access to individual tools which is ideal for students or personal users focused on a single analysis. If you‚Äôre a scientist or assistant professor involved in active research, explore our Individual Plan for advanced access."
  },
  {
    "objectID": "plans.html#institutional-license",
    "href": "plans.html#institutional-license",
    "title": "RAISINS",
    "section": "üè´ Institutional License",
    "text": "üè´ Institutional License\nWe also offer comprehensive Institutional Licenses to support collaborative research and academic teaching. These plans provide Access to all RAISINS apps for a fixed number of users within a department, college, or university. Whether you‚Äôre supervising a small team or managing a large institution, there‚Äôs a plan designed to fit your scale and scope.\n\nInstitutional licenses are ideal for teaching labs, research projects, and campus-wide adoption of RAISINS.\n\n\nExplore Institutional License plans üëá\n\n\n\n  \n  RAISINS Subscription Plans\n  \n  \n\n  \n\n  Choose the flavour that fits your institution‚Äôs taste for research\nAll amounts listed are exclusive of GST.\n\n  \n\n   \n\n  \n    ü´ê\n    Blueberry RAISINS - Individual Plan\n    \n       Users: 1\n       Full access to RAISINS modules\n       250 user hours per module\n       1 year validity\n    \n    ‚Çπ20,000 (Excl. GST)\n    International: $500 USD\n  \n  Enquire\n\n\n\n\n  \n    üçì\n    Strawberry RAISINS - Department Pack\n    \n       Users: 5\n       Full access to RAISINS modules\n       250 user hours per module per user\n       1 year validity\n    \n    ‚Çπ80,000 (Excl. GST)\n    International: $1800 USD\n  \n  Enquire\n\n\n\n\n  \n    üçí\n    Cherry RAISINS - College Basic Bundle\n    \n       Users: 25\n       Full access to RAISINS modules\n       250 user hours per module per user\n       1 year validity\n    \n    ‚Çπ3,50,000 (Excl. GST)\n    International: $8800 USD\n  \n  Enquire\n\n\n\n\n  \n    üçá\n    Blackcurrant RAISINS - University Plan\n    \n       Users: 100\n       Full access to RAISINS modules\n       250 user hours per module per user\n       1 year validity\n    \n    ‚Çπ9,50,000 (Excl. GST)\n    International: $24,500 USD\n  \n  Enquire\n\n\n\n  \n\n  \n    üì© Write to us at support@statoberry.com to get a custom flavour for your organization."
  },
  {
    "objectID": "samplesize.html",
    "href": "samplesize.html",
    "title": "RAISINS",
    "section": "",
    "text": "Here you can calculate the sample size required for your survey or study based on your desired confidence level, margin of error, and expected proportion. This tool supports both large (infinite) and finite population scenarios, helping you determine how many responses you need to achieve statistically valid results.\n\n\n\n  \n    üéØ Interactive Sample Size Calculator\n\n    \n      Population Size (N)\n      üí°\n    \n    \n    \n      The (estimated or guess) total number of people or units in the population you're studying. If it's unknown or very large, leave it blank and the calculator will assume an infinite population.\n    \n\n    \n      Margin of Error (%) [E]\n      üí°\n    \n    \n      \n      5%\n    \n    \n      A smaller margin means a larger required sample size. A 5% margin means your result may vary ¬±5% from the true value.\n    \n\n    \n      Expected Proportion (p)\n      üí°\n    \n    \n      \n      0.50\n    \n    \n      This is your best guess of the proportion of the population with the trait. Use 0.5 if unsure ‚Äî it gives the largest required sample size.\n    \n\n    \n      Confidence Level (Z)\n      üí°\n    \n    \n      80%\n      85%\n      90%\n      95%\n      99%\n    \n    \n      How confident you want to be in the result. 95% means the result will be within margin of error in 95 of 100 surveys.\n    \n\n    \n\n    \n  üìò Explanation: How Sample Size is Calculated\n  \n    Determining the appropriate sample size is a critical step in designing any statistical survey or research study. When you want to estimate a proportion (for example, the percentage of farmers adopting a new technology), the required sample size depends on four main factors: the confidence level, the expected proportion, the acceptable margin of error, and the total population size.\n  \n\n  üîπ 1. Formula for Infinite Population\n  The basic formula for estimating sample size when the population is assumed to be very large (effectively infinite) is:\n  \n    n‚ÇÄ = (Z¬≤ √ó p √ó (1 ‚àí p)) / E¬≤\n  \n  \n    n‚ÇÄ: The minimum required sample size assuming an infinite population.\n    Z: The Z-score (standard normal value) corresponding to the desired confidence level.\n      For example: Z = 1.96 for 95% confidence, Z = 1.64 for 90%, etc.\n    p: The expected proportion of the population having the characteristic of interest.\n      Use 0.5 if unknown, as it gives the most conservative (largest) sample size.\n    E: The desired margin of error (as a decimal).\n      For example: E = 0.05 for ¬±5% precision.\n  \n\n  üîπ 2. Finite Population Correction (FPC)\n  If your population size N is not very large, a correction to adjust the sample size is applied. The corrected formula is:\n  \n    n = n‚ÇÄ / [1 + ((n‚ÇÄ ‚àí 1) / N)]\n    \n  \n  ‚úÖ Key Insights\n  \n    A smaller margin of error increases the required sample size.\n    Higher confidence levels require larger samples.\n    If you are unsure about the expected proportion, use p = 0.5 to get the largest (safest) estimate.\n    Finite population correction is important when your total population is relatively small.\n  \n\n  \n    üìö Reference: Cochran, W. G. (1977). Sampling Techniques (3rd ed.). New York: John Wiley & Sons."
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "RAISINS learning hub",
    "section": "",
    "text": "welcome\n    RAISINS learning hub\n    \n      your comprehensive space for mastering data analysis with RAISINS-interactive guides, real examples,\n      and clear, step-by-step guidance that make statistics simpler and more accessible.\n    \n\n    \n      step-by-step\n      interactive\n      smooth reading\n      statistics, sweeter\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started\n\n\n\nGetting-started\n\n\n\nHere we introduces users to the RAISINS platform and guides them through the essential steps to get started. It explains where to begin, how to start an analysis, upload data, and follow the data preparation guidelines‚Ä¶. Read more ‚Ä¶\n\n\n\n\n\nNov 7, 2025\n\n\nDr.¬†Pratheesh P Gopinath, Hisham M\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Cluster Analysis\n\n\n\nMultivariate\n\nSocial-science\n\n\n\nHierarchical Cluster Analysis (HCA) widely used in Agricultural research ‚Ä¶ Read more ‚Ä¶\n\n\n\n\n\nNov 1, 2025\n\n\nJithin Chandran, Pratheesh P Gopinath, Akhila P. S.\n\n28 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "RAISINS",
    "section": "",
    "text": "Welcome to Statoberry LLP. By accessing and using our website, you agree to comply with and be bound by the following terms and conditions. If you disagree with any part of these terms, please do not use our services."
  },
  {
    "objectID": "terms.html#introduction",
    "href": "terms.html#introduction",
    "title": "RAISINS",
    "section": "",
    "text": "Welcome to Statoberry LLP. By accessing and using our website, you agree to comply with and be bound by the following terms and conditions. If you disagree with any part of these terms, please do not use our services."
  },
  {
    "objectID": "terms.html#services-provided",
    "href": "terms.html#services-provided",
    "title": "RAISINS",
    "section": "2 Services Provided",
    "text": "2 Services Provided\nStatoberry LLP offers data analysis services, database management systems, website building, and other related services. Our services are provided as-is, and we reserve the right to make changes or discontinue services without prior notice."
  },
  {
    "objectID": "terms.html#user-obligations",
    "href": "terms.html#user-obligations",
    "title": "RAISINS",
    "section": "3 User Obligations",
    "text": "3 User Obligations\nBy using our website, you agree not to:\n\nUse our services for any unlawful or fraudulent activities.\nInfringe on the rights of other users.\nUpload or transmit any harmful or malicious software."
  },
  {
    "objectID": "terms.html#intellectual-property",
    "href": "terms.html#intellectual-property",
    "title": "RAISINS",
    "section": "4 Intellectual Property",
    "text": "4 Intellectual Property\nAll content on this site, including text, graphics, logos, and software, is the property of Statoberry LLP or its content suppliers. Unauthorized use of the content may result in legal action."
  },
  {
    "objectID": "terms.html#limitation-of-liability",
    "href": "terms.html#limitation-of-liability",
    "title": "RAISINS",
    "section": "5 Limitation of Liability",
    "text": "5 Limitation of Liability\nStatoberry LLP will not be liable for any damages resulting from the use or inability to use our services. This includes, without limitation, incidental and consequential damages."
  },
  {
    "objectID": "terms.html#termination",
    "href": "terms.html#termination",
    "title": "RAISINS",
    "section": "6 Termination",
    "text": "6 Termination\nWe reserve the right to terminate your access to our website and services if you breach these terms."
  },
  {
    "objectID": "terms.html#governing-law",
    "href": "terms.html#governing-law",
    "title": "RAISINS",
    "section": "7 Governing Law",
    "text": "7 Governing Law\nThese terms shall be governed by and construed in accordance with the laws of India."
  },
  {
    "objectID": "terms.html#changes-to-terms",
    "href": "terms.html#changes-to-terms",
    "title": "RAISINS",
    "section": "8 Changes to Terms",
    "text": "8 Changes to Terms\nWe may modify these terms at any time. Users will be informed of significant changes, and continued use of the site constitutes acceptance of the new terms."
  },
  {
    "objectID": "terms.html#contact-us",
    "href": "terms.html#contact-us",
    "title": "RAISINS",
    "section": "9 Contact Us",
    "text": "9 Contact Us\nIf you have any questions about these terms, please contact us at statistician@statoberry.com."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html",
    "href": "tutorial/Intro/raisinsIntro.html",
    "title": "Getting Started",
    "section": "",
    "text": "It all began with a simple idea: to make data analysis accessible, accurate, and stress-free for the agricultural research community. Back in 2020, we introduced GRAPES (General R-based Analysis Platform Empowered by Statistics) www.kaugrapes.com a humble, R shiny -based tool designed for the students and researchers of Kerala Agricultural University. Little did we know then, it would grow into a trusted companion for more than 200,000 users worldwide and earn citations in over 150 indexed journals.\nBut growth brings new challenges. GRAPES, originally tailored for local use, struggled to keep pace with the scale and expectations of a global audience. We knew we needed to build something more robust, something that could not just meet, but anticipate the needs of modern agricultural research.\nThat dream took shape in 2024, when a spirited team of young statisticians came together to form STATOBERRY LLP, under the mentorship of Dr.¬†Pratheesh P. Gopinath, Head of the Department of Agricultural Statistics, College of Agriculture, Vellayani. From this dynamic collaboration emerged online web application for reseacrh data analysis in agriculture RAISINS-R and AI Solutions in INferential Statistics. RAISINS can be accessed at www.raisins.live\nRAISINS is not just another software. It is a complete ecosystem that brings together the power of R, Python, and AI in a seamless, point-and-click interface. Whether you‚Äôre working on design of experiments, social science surveys, or advanced statistical genetics, RAISINS simplifies the process, automates best practices, and delivers publication-ready results without requiring you to write a single line of code."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#raisins",
    "href": "tutorial/Intro/raisinsIntro.html#raisins",
    "title": "Getting Started",
    "section": "",
    "text": "It all began with a simple idea: to make data analysis accessible, accurate, and stress-free for the agricultural research community. Back in 2020, we introduced GRAPES (General R-based Analysis Platform Empowered by Statistics) www.kaugrapes.com a humble, R shiny -based tool designed for the students and researchers of Kerala Agricultural University. Little did we know then, it would grow into a trusted companion for more than 200,000 users worldwide and earn citations in over 150 indexed journals.\nBut growth brings new challenges. GRAPES, originally tailored for local use, struggled to keep pace with the scale and expectations of a global audience. We knew we needed to build something more robust, something that could not just meet, but anticipate the needs of modern agricultural research.\nThat dream took shape in 2024, when a spirited team of young statisticians came together to form STATOBERRY LLP, under the mentorship of Dr.¬†Pratheesh P. Gopinath, Head of the Department of Agricultural Statistics, College of Agriculture, Vellayani. From this dynamic collaboration emerged online web application for reseacrh data analysis in agriculture RAISINS-R and AI Solutions in INferential Statistics. RAISINS can be accessed at www.raisins.live\nRAISINS is not just another software. It is a complete ecosystem that brings together the power of R, Python, and AI in a seamless, point-and-click interface. Whether you‚Äôre working on design of experiments, social science surveys, or advanced statistical genetics, RAISINS simplifies the process, automates best practices, and delivers publication-ready results without requiring you to write a single line of code."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#gettingStarted",
    "href": "tutorial/Intro/raisinsIntro.html#gettingStarted",
    "title": "Getting Started",
    "section": "2 Getting started",
    "text": "2 Getting started\n\nGo to https://www.raisins.live/\nClick on Start in the home page of RAISINS as shown in Figure¬†1 or simply scroll down\n\n\n\n\n\n\n\nFigure¬†1: RAISINS Home page\n\n\n\n\nYou will be taken to the list of available modules in RAISINS under the Data Analysis section, as shown in Figure¬†2. Click on any module to get started. Each module is self-contained and covers every aspect of its respective analysis from data input to interpretation. All RAISINS modules follow a consistent layout and workflow, making it easy for users to navigate and perform different analyses without having to learn each one separately.\n\n\n\nWhat is a module?\n\nIn RAISINS, each complete analytical tool or statistical application such as t-test, Regression Analysis, or CRD is presented as a separate, self-contained application, which can be accessed indvidually. These individual applications are referred to as modules.\n\n\n\n\n\n\n\nFigure¬†2: Analysis modules are listed section-wise in RAISINS\n\n\n\n\nClicking on any analysis module will open up a window like that shown in Figure¬†3. Each module has three login options.\nPreview Mode: Before subscribing to any module, you can explore RAISINS using the preview mode. Simply enter your email ID, and an OTP will be sent to your inbox. Use this OTP to log in and test any module with the built-in sample datasets. For your convenience, the OTP generated for your account remains valid until it refreshes at 12 a.m. and 12 p.m. (IST). This means you can revisit and explore multiple modules within that time window using the same OTP, without needing to generate a new one. Preview mode is an excellent feature for teaching, demonstrations, and learning how RAISINS works before making a subscription.\nGet Started: This is the primary login option for users who have subscribed to any RAISINS module. Simply log in using your registered email ID and the password you created during registration. Once logged in, you can upload your own datasets, and begin your analysis seamlessly.\nInstitutional Login: If you are subscribed to all RAISINS modules through the Unlimited Plan or have access via an institutional license, you can log in using this option. An OTP will be sent to your registered email ID, which you can use to log in. Like in preview mode the OTP generated for your account remains valid until it refreshes at 12 a.m. and 12 p.m. (IST) and can be used to access all the modules included in your institutional plan.\n\n\n\n\n\n\n\nAll OTPs generated in RAISINS are refreshed only at 12 a.m. and 12 p.m. (IST) You don‚Äôt need to generate a new OTP each time ‚Äî the same OTP can be used to access any module until the next refresh, for both Preview Mode and Institutional Login.\n\n\n\n\n\n\n\n\n\nFigure¬†3: RAISINS module window\n\n\n\n\nIf you are a beginner, you might sometimes feel unsure about which analysis tool to choose or what tool is best for your data and research. That‚Äôs exactly where a statistician‚Äôs guidance becomes valuable and RAISINS offers you three simple ways to get it.\n\nAsk our AI Statistician, RA-One (RAISINS Assistant One): a free, intelligent assistant that helps you choose the right analysis and guides you through each step. See the instructions for accessing RA-One here üëâ Section¬†8\nJoin our Discussion Corner: if you have a GitHub account, you can post your queries and interact with the RAISINS team and other users in an open, collaborative forum.\n\nEmail us directly at support@statoberry.com: our team will respond personally and help you with your analysis-related questions. In this option we will get back to you in a maximum of 3 working days."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-appWork",
    "href": "tutorial/Intro/raisinsIntro.html#sec-appWork",
    "title": "Getting Started",
    "section": "3 How each module works!",
    "text": "3 How each module works!\nThe workflow of all modules or applications within RAISINS follows a uniform and intuitive pattern. This consistency is intentional as it ensures that users can move between different tools with ease and without confusion. The process is straightforward:\n\nArrange your data file in the prescribed format (see Section¬†4).\nClick Browse and upload your dataset.\nClick Run Analysis and RAISINS will handle the rest!\n\n\n\n\n\n\n\n\nRAISINS workflow\n\n\nA sample module(application) window is shown in Figure¬†4 and Figure¬†5.\n\n3.1 Four main tabs\nFrom Figure¬†4, you can see how an application in RAISINS appears. Every application follows a similar structure with four main tabs, each serving a distinct purpose. The four tabs are:\n1 Analysis:\nThis is the main workspace where the analysis is performed. You can upload your data file here, select the required variables or columns, and click Run Analysis to instantly view the results.\n2 Create Data:\nThis tab allows you to generate the required data format by providing the specified parameters. You can download the file in .xlsx or .csv format and use it for analysis. This feature is especially helpful if you‚Äôre unsure about the correct data structure.\n3 Datasets:\nEach module includes built-in model datasets that you can use for testing. You can also download these datasets to understand how your own data should be arranged before uploading.\n4 User:\nThis tab provides details about your subscription and usage statistics. You can also view or download invoices related to your RAISINS access here.\n\n\n\n\n\n\nFigure¬†4: Inside a module in RAISINS\n\n\n\n\n\n\n\n\n\nAll modules in RAISINS follow a similar workflow. Just upload your file in the prescribed format and click Run Analysis, as shown in Figure¬†5.\nüëâ You only need to be careful while arranging your data. You can refer to the model datasets in Datasets tab of each application or use the Create Data tab.\nSee Section¬†4 for key points to remember when preparing your file for upload.\n\n\n\n\n\n\n\n\n\nFigure¬†5: Click on Run Analysis after uploading the file"
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-prepfile",
    "href": "tutorial/Intro/raisinsIntro.html#sec-prepfile",
    "title": "Getting Started",
    "section": "4 Preparing your file",
    "text": "4 Preparing your file\nIn RAISINS you can upload file in xls, xlsx or csv format. But we recommend a csv format as it will be much lighter.\n\n4.1 Preparing your data\nYou can create a file for upload in two simple ways:\n\nMicrosoft Excel (.xlsx): create a single sheet with a clear header row and save as .xlsx or export to .csv.\n\nCreate data (recommended for beginners): inside each RAISINS module, open the Create data tab and enter the requested inputs in the sidebar.\nExample: In the CRD module, provide the number of treatments, replications, and characters. RAISINS will generate a ready-to-fill .csv template that you can download, enter values, and upload for analysis.\n\n\n\n\n\n\n\nDetailed instructions on how to prepare data files are given inside each module and also in its corresponding tutorial. If you‚Äôre unsure about the required structure, download a model dataset from the Datasets tab in that module and mirror its layout.\n\n\n\n\n\n4.2 Rules to follow\n\nUse a single header row with clean column names.\n\nData should begin in cell A1, with no blank rows above the header.\n\nOne variable per column, one record per row.\n\nSave as .csv or .xlsx (both supported).\n\n\n\n\n4.3 Column naming: do‚Äôs and don‚Äôts\n\n\n\n\n\n\n\n\n\n‚úÖ Do\n‚ùå Don‚Äôt\nüí° Why\n\n\n\n\nUse short, clear names: yield, treatment, rep\nLong or vague names: this_is_my_final_corrected_yield_value\nShort names read better in tables and plots\n\n\nSeparate words with _ or .: plant_height\nUse spaces: plant height\nSpaces can break code and labels\n\n\nUse letters first: r1_yield\nStart with a number: 1yield\nColumn names shouldn‚Äôt start with digits\n\n\nAvoid symbols: perc_mortality\nUse symbols: percentage mortality (%)\nSymbols like %, # cause parsing issues\n\n\n\n\n\n\n\n4.4 A few column name examples to note\n\npercentage mortality (%) ‚Üí perc_mortality\n\nPlant height (cm) ‚Üí plant_height_cm\n\nT1 Yield ‚Üí t1_yield\n\n\n\n\n4.5 Minimal CSV example\n\n\n\n\n\n\nFigure¬†6: Minimal example data file"
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-plogin",
    "href": "tutorial/Intro/raisinsIntro.html#sec-plogin",
    "title": "Getting Started",
    "section": "5 Preview login",
    "text": "5 Preview login\nThis is an excellent feature we have introduced in RAISINS. It allows anyone to freely explore any module without registration, promoting open access and enabling global peer review, thereby enhancing transparency and credibility.\nWhen you click on a module,in the login window that opens up- simply select the Preview Mode button as shown in Figure¬†3. In Figure¬†7, you can see how to access the preview mode using an OTP. As illustrated in Figure¬†7 (a), if you already generated an OTP in another module, note that it refreshes only at 12 a.m. and 12 p.m. each day. In such cases, you can use the button I Already Have OTP to proceed. Which will take you to the screen shown in Figure¬†7 (b), where you have to enter the OTP and get inside the app.\n\n\n\n\n\n\n\n\n\n\n\n(a) Enter your mail id to receive OTP\n\n\n\n\n\n\n\n\n\n\n\n(b) Enter the OTP to acces the app\n\n\n\n\n\n\n\nFigure¬†7: Preview mode in RAISINS\n\n\n\nOnce inside the app, you can explore all the available features, analyses, and plots using the inbuilt datasets. This allows you to fully experience how each module functions. However, please note that uploading your own dataset is not permitted in preview mode. A sample view of the preview mode interface is shown in Figure¬†8.\n\n\n\n\n\n\nFigure¬†8: Inside preview mode"
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-Instlogin",
    "href": "tutorial/Intro/raisinsIntro.html#sec-Instlogin",
    "title": "Getting Started",
    "section": "6 Institutional login",
    "text": "6 Institutional login\nThis login feature is available to users who have subscribed to all RAISINS modules through the Individual Plan (unlimited plan), any customized plan, or whose institution is registered with us. You can access any module or only those modules included in your plan by clicking the Institutional Login button in the login window of the selected module, as shown in Figure¬†3.\nAn OTP will be sent to your registered email address for authentication. Similar to the preview mode, all OTPs are refreshed only at 12 a.m. and 12 p.m., allowing you to reuse the same OTP for logging into other modules within that time frame.\nYou can view the available subscription plans for RAISINS here"
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-modlogin",
    "href": "tutorial/Intro/raisinsIntro.html#sec-modlogin",
    "title": "Getting Started",
    "section": "7 Module login",
    "text": "7 Module login\nRAISINS allows users to access each module independently through subscribing that particular module only. A feature designed with students in mind. If you have subscribed to a single module, you can start using it directly by clicking the Get Started button, as shown in Figure¬†3."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-RAone1",
    "href": "tutorial/Intro/raisinsIntro.html#sec-RAone1",
    "title": "Getting Started",
    "section": "8 AI Assistant (RA-One)",
    "text": "8 AI Assistant (RA-One)\nRAISINS features a 24 √ó 7 AI statistical assistant named RAISINS Assistant-One (RA-One), which is freely accessible to all users. Powered by GPT-4.5 Turbo and further enhanced through Retrieval-Augmented Generation (RAG), RA-One functions like a trained master‚Äôs-level statistician. He can clarify your statistical doubts, guide your analysis, and even recommend the most suitable RAISINS module or tool for your dataset.\n\n\n\n\n\n\n\n\nAccessing RA-One\n\n\n\n\n\n\n\n\n\n\n(a) RA-One bot interface\n\n\n\n\n\n\n\nFigure¬†9: RA-One: AI powered assistant in RAISINS"
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-privacy",
    "href": "tutorial/Intro/raisinsIntro.html#sec-privacy",
    "title": "Getting Started",
    "section": "9 Data privacy",
    "text": "9 Data privacy\nAt RAISINS, data privacy and security are of utmost importance. We do not store or retain any data that users upload for analysis. Each session in RAISINS runs on a temporary cloud instance, meaning that all uploaded data, results, and computations exist only for the duration of your active session. Once you log out, the instance is automatically terminated, and all associated data is permanently deleted. This ensures complete confidentiality and eliminates any risk of unauthorized access.\n\n\n\n\n\n\nwe recommend that you download your analysis results, plots, and graphs immediately after completing your analysis session as we don‚Äôt store anything."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-issue",
    "href": "tutorial/Intro/raisinsIntro.html#sec-issue",
    "title": "Getting Started",
    "section": "10 Issues and support",
    "text": "10 Issues and support\nWe are always here to help you with any data-related queries or issues you may face while using RAISINS. You can reach us directly through the contact us in RAISINS website or by emailing us at support@statoberry.com.\nIf you encounter any bugs, you can also report them directly through our GitHub repository using the ‚ÄúReport a Bug‚Äù button available on the homepage (for users with a GitHub account) or notify us by mail.\nWe usually respond within one hour during office hours (Monday to Friday) to any issue raised. You can also contact us via WhatsApp using the number provided on our contact us page.\n\nDuring holidays, our response time might be a bit longer but we‚Äôll still do our best to get back to you as soon as possible."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-multiaccess",
    "href": "tutorial/Intro/raisinsIntro.html#sec-multiaccess",
    "title": "Getting Started",
    "section": "11 Multiple access control",
    "text": "11 Multiple access control\nPlease note that, for your security, RAISINS does not allow simultaneous access from multiple devices using the same account. If your account is detected as being used on more than one device at the same time, the system will automatically identify it and send a notification alert. So please keep your credentials safe."
  },
  {
    "objectID": "tutorial/Intro/raisinsIntro.html#sec-conclusion",
    "href": "tutorial/Intro/raisinsIntro.html#sec-conclusion",
    "title": "Getting Started",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nRAISINS was born from an academic vision: a collective effort mentored by Department of Agricultural Statistics, College of to democratize statistics. It stands not as a commercial product, but as a bridge connecting knowledge with practice, empowering every researcher to analyze, interpret, and share data with confidence.\nWe are with you at every step of your research journey- guiding, supporting, and growing together with the community we serve. Through our academic outreach, RAISINS proudly supports the open-access journal Journal of Sustainable Technology in Agriculture (JOSTA), which remains free of article processing charges (APC), ensuring that knowledge stays open and accessible to all.\nIn the same spirit, RAISINS also sustains the cloud infrastructure of GRAPES, allowing students and researchers continued free access to powerful statistical learning tools. Together, we strive to make research simpler, transparent, and inclusive for a future where statistics belongs to everyone."
  },
  {
    "objectID": "tutorial.html#raisins-learning-hub",
    "href": "tutorial.html#raisins-learning-hub",
    "title": "RAISINS",
    "section": "",
    "text": "Welcome to the RAISINS Learning Hub, your comprehensive guide to mastering data analysis with RAISINS. Here, you‚Äôll find a collection of interactive guides and walkthroughs designed to help you explore, analyze, and visualize data effectively using our cloud-based tools.\n\n\n\n\n\n\n\n\n\n\n\n\nGetting-started\n\n\n\nHere we introduces users to the RAISINS platform and guides them through the essential steps to get started. It explains where to begin, how to start an analysis, upload data, and follow the data preparation guidelines‚Ä¶. Read more ‚Ä¶\n\n\n\n\n\nNov 7, 2025\n\n\nDr.¬†Pratheesh P Gopinath, Hisham M\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate\n\nSocial-science\n\n\n\nHierarchical Cluster Analysis (HCA) widely used in Agricultural research ‚Ä¶ Read more ‚Ä¶\n\n\n\n\n\nNov 1, 2025\n\n\nJithin Chandran, Pratheesh P Gopinath, Akhila P. S.\n\n26 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "plans.html#sec-indv",
    "href": "plans.html#sec-indv",
    "title": "RAISINS",
    "section": "Indvidual Plan - unlimited",
    "text": "Indvidual Plan - unlimited\n\n\n\n  \n  RAISINS Individual Plan Offer\n  \n  \n\n\n\n  \n    \n      ü´ê\n      Individual Plan - unlimited\n    \n\n    \n       Single user access\n       Full access to all RAISINS modules\n       Unlimited usage\n       Valid for 1 year\n    \n\n    \n      ‚Çπ20,000 ‚Çπ12,000 for a year\n      üåç International: $136 USD\n    \n\n    üéâ Save ‚Çπ8,000 on this offer!\n    Payable in 4 easy quarterly installments\n\n    \n      Still unsure? Get a demo or test access ‚Äî contact us below!\n    \n\n    \n      Get Test Access / Enquire\n    \n  \n\n\n\n\n\n\n\n\n\nTipüí°Institutional Plan\n\n\n\nüí° If you are affiliated with an academic or research institution and require full access to all RAISINS modules, please refer to the Institutional License section below and encourage your institution to partner with us. See Institutional License for more details."
  },
  {
    "objectID": "plans.html#sec-inst",
    "href": "plans.html#sec-inst",
    "title": "RAISINS",
    "section": "üè´ Institutional License",
    "text": "üè´ Institutional License\nWe also offer comprehensive Institutional Licenses to support collaborative research and academic teaching. These plans provide Access to all RAISINS apps for a fixed number of users within a department, college, or university. Whether you‚Äôre supervising a small team or managing a large institution, there‚Äôs a plan designed to fit your scale and scope.\n\nInstitutional licenses are ideal for teaching labs, research projects, and campus-wide adoption of RAISINS.\n\n\n\n  Contact us for customized plans\n\n\n\n\n\n  üèõÔ∏è institutions with access"
  },
  {
    "objectID": "index.html#raisins-doodle-",
    "href": "index.html#raisins-doodle-",
    "title": "Welcome to RAISINS",
    "section": "0.1 RAISINS DOODLE‚Äî-",
    "text": "0.1 RAISINS DOODLE‚Äî-\n\n\n\n  \n    \n      \n      \n        \n        \n        \n      \n\n      \n      \n        R\n        A\n        I\n        S\n        I\n        N\n        S\n      \n\n      \n      \n        \n        \n          \n          \n          \n            \n            \n            \n            \n            \n          \n        \n\n        \n        \n          \n          \n          \n          \n        \n\n        \n        \n          \n            \n              \n              \n            \n          \n          \n          \n          \n          \n        \n\n        \n        \n          \n          \n        \n\n        \n        \n          \n          \n          \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n\n      \n      11 ‚Ä¢ 11 ‚Ä¢ 2025\n\n      \n      \n        National education day\n        India ‚Ä¢ celebrate learning, teachers & ideas\n      \n    \n  \n  National Education Day, India ‚Äî 11 November 2025. Celebrating learners, teachers, and the power of ideas with RAISINS."
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#getting-started",
    "href": "tutorial/cluster/Clustertutorial.html#getting-started",
    "title": "Hierarchical Cluster Analysis",
    "section": "1 Getting Started",
    "text": "1 Getting Started\nRAISINS (R and AI Solutions in INferential Statistics) is a cloud-based platform that allows you to perform statistical analyses in R and Python without writing a single line of code. It runs entirely online which needs no downloads or installations and seamlessly integrates the capabilities of R, Python, and AI to deliver powerful yet user-friendly analytical tools.\nLearn more about RAISINS here.\nTo get started with cluster analysis, visit www.raisins.live, navigate to Cluster Analysis under Data Analysis as shown in Figure¬†1.\n\n\n\n\n\n\nFigure¬†1: Cluster analysis in RAISINS"
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#hierarchical-cluster-analysis-hca",
    "href": "tutorial/cluster/Clustertutorial.html#hierarchical-cluster-analysis-hca",
    "title": "Hierarchical Cluster Analysis",
    "section": "2 Hierarchical Cluster Analysis (HCA)",
    "text": "2 Hierarchical Cluster Analysis (HCA)\nThe purpose of hierarchical agglomerative cluster analysis is to group similar items or observations together based on how close or related they are. It helps you find natural groupings or patterns in your data without knowing the number of groups in advance.\nIn this method, the analysis starts with each item as its own separate group. Then, the most similar groups are joined step by step until all items come together into one big group. This process is shown using a tree-like diagram called a dendrogram, which helps you see how the groups were formed and decide where to ‚Äúcut‚Äù the tree to get the right number of clusters.\nThis method is useful when you want to explore your data and understand which items are more alike based on their features or values.\n\n\n\n\n\n\nTipIn short\n\n\n\nHierarchical agglomerative cluster analysis helps group similar crops, varieties, or treatments based on their measured characters. The grouping is shown using a dendrogram, and since it is exploratory, users can decide how many clusters to keep - we will discuss finding the optimal number of clusters in the coming sessions."
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#lets-start-with-an-example",
    "href": "tutorial/cluster/Clustertutorial.html#lets-start-with-an-example",
    "title": "Hierarchical Cluster Analysis",
    "section": "3 Let‚Äôs start with an example",
    "text": "3 Let‚Äôs start with an example\nWe will explain hierarchical cluster analysis (HCA) using an example. Let‚Äôs consider a dataset with 30 genotypes and four variables - Yield, Plant_Height, Pods_per_Plant, and Seed_Weight. Our aim is to perform cluster analysis to group the genotypes based on these characters.\nIf you are from the social sciences, your goal will be similar, to group respondents or study units based on the variables under study. The arrangement of the data is shown in the Figure¬†2. Data organized in the same way in MS Excel can be directly uploaded to RAISINS for analysis. For more details on data preparation see Section¬†4.\nTwo terms that we will use frequently are Labels and Variables. In our example, the Labels refer to the genotypes, and the Variables are the four traits mentioned earlier - Yield, Plant_Height, Pods_per_Plant, and Seed_Weight.\n\n\n\n\n\n\nFigure¬†2: Model dataset: 30 genotypes and four characters in MS Excel"
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#sec-prepdata",
    "href": "tutorial/cluster/Clustertutorial.html#sec-prepdata",
    "title": "Hierarchical Cluster Analysis",
    "section": "4 How to Prepare Your Data?",
    "text": "4 How to Prepare Your Data?\nArranging data for upload in RAISINS is very simple. Prepare your data exactly like the one shown in Figure¬†2, using a single-sheet Excel file. Make sure no blank rows are left above, and all columns have proper names. That‚Äôs it - your file is ready to upload.\nStill if you have doubt read below:-\nTo prepare your dataset for analysis in RAISINS, you have two options:\n\n\nCreating dataset in MS Excel\n\n\nOpen a new Microsoft Excel file, use single sheet only.\nStart with Cell A1: begin entering data from cell A1. Do not leave any blank rows above.\nFirst Row - Column Names: The first row must contain the column names.\nColumn 1: Enter treatment/labels. There should not be any repetition in the label ID‚Äôs. If there is replications, you can use the mean values.\nFrom Column 2 Onwards: Enter the names of each variable under study as separate columns (e.g.yield, Plant_Height, Pods_per_Plant, and Seed_Weight). You can give any names to the columns.\n\nSee Figure¬†2 showing how the prepared Excel file for upload should look like\nIf you have any doubt in saving a file as csv or some basics of data preperation read our tutorial on getting started here\n\n\n\nCreating your dataset directly within the RAISINS app\n\n\nNavigate to Create Data Tab: Click on the Create Dataset tab in the main menu at the top of the app.\nSpecify Details: Enter the levels of Factor A, Factor B, Factor C and number of characters under study in the window that opens.\nThen click create\nModel Data Entry File: A template for data entry will be generated. You can:\n\nDirectly enter your data into this template.\nOr, copy-paste data from an existing Excel file.\n\nDownload as CSV: Once the data is entered, click on the Download CSV File button. The downloaded CSV file can be uploaded for analysis in Analysis tab.\n\n\n\n\n\n\n\nFigure¬†3: Creating Data in RAISINS"
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#running-analysis",
    "href": "tutorial/cluster/Clustertutorial.html#running-analysis",
    "title": "Hierarchical Cluster Analysis",
    "section": "5 Running Analysis",
    "text": "5 Running Analysis\nNow upload the prepared file by clicking Browse in the sidebar of the Analysis tab (see Figure¬†4).\n\n\n\n\n\n\nFigure¬†4: Analysis tab\n\n\n\nWhen the file is uploaded, options to select the variables and labels appear. Select appropriate column under Labels and variables, see Figure¬†5\n\n\n\n\n\n\nFigure¬†5: Uploading the data file\n\n\n\nOnce you click the Run Analysis button, all relevant results and outputs appear instantly-leaving no room for confusion. Now see the result for our example data in the form of a dendrogram Figure¬†6.\n\n\n\n\n\n\nNoteDendrogram\n\n\n\nA dendrogram is a tree-like diagram that visually represents how hierarchical clustering groups data step by step, allowing you to explore relationships and cluster formation within a dataset. A dendrogram displays the process of merging (or sometimes splitting) clusters during hierarchical cluster analysis\n\n\n\nStep-by-Step: How to read a Dendrogram ?\n\n\nStart at the bottom - Each leaf represents a single observation or sample.\nFollow the lines upward - The first merges connect the most similar items. These connections indicate that the points or small clusters joined are very close based on the chosen similarity measure.\nMerging clusters - As you go higher, clusters are combined with others, and each branching indicates additional clustering - think of it as building a family tree for your data.\nBranch heights - The height at which branches merge tells you how similar the merged groups are-the lower the connection, the more similar the groups.\nDeciding number of clusters - By ‚Äúcutting‚Äù the tree horizontally at a given level (distance threshold), you divide the data into clusters-each branch below the cut line forms a cluster.\n\n\n\n\n\n\n\n\n\n\nFigure¬†6: Dendrogram of the model dataset\n\n\n\nIn RAISINS, the optimal number of clusters is automatically identified using the elbow method.Other options can be seen in optimal clusters tab. In this example, the method suggests two clusters, which are shown in red and blue in Figure¬†6.\nYou can also change the number of clusters as needed. RAISINS provides many additional options that the experimenter can choose from depending on the type of data and research objective. The platform is highly interactive and guides you on what to use and when, making the clustering process easier and more intuitive, see Section¬†6."
  },
  {
    "objectID": "tutorial/cluster/Clustertutorial.html#sec-detail",
    "href": "tutorial/cluster/Clustertutorial.html#sec-detail",
    "title": "Hierarchical Cluster Analysis",
    "section": "6 RAISINS clustering options explained",
    "text": "6 RAISINS clustering options explained\nIn Figure¬†7, you can see the detailed view of the Analysis tab, along with explanations of what each option does. This section helps you understand the purpose of every setting so you can select the most appropriate ones for your data and analysis needs.\n\n\n\n\n\n\nFigure¬†7: Run Analysis Tab in RAISINS\n\n\n\n\n6.1 Different plot types\nRAISINS offers a variety of plots related to Hierarchical Cluster Analysis, ensuring that all key visualizations are easily accessible. Each plot includes a gear icon at the top-left corner that lets you customize its appearance to suit your needs. You can also download these plots in high-quality PNG (300 dpi), TIFF, or PDF formats for use in your reports or presentations (see Figure¬†7).\nFrom Figure¬†8 to Figure¬†15, you can see the different types of dendrograms available in RAISINS. Each one is visually illustrated and accompanied by a clear, insightful description below, making it easy to understand the structure, grouping, and interpretation of each clustering pattern.\n\n\n\n\n\n\n\n\n\nFigure¬†8: Color coded clusters (k = 2)\nA color-coded cluster dendrogram is an enhanced version of the basic dendrogram where branches or clusters are colored differently to clearly distinguish the groupings formed by hierarchical clustering. This makes interpreting the tree much easier, especially for large datasets or when clusters overlap visually.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9: Vertical Dendrogram (k = 2)\nA vertical dendrogram is a classic, user-friendly way to display hierarchical clustering results with the data points along the bottom and cluster merges represented by upward vertical lines, enabling easy interpretation of cluster hierarchy and distances.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†10: Horizontal Dendrogram (k = 2)\nA horizontal dendrogram is a tree-like diagram used to visualize hierarchical clustering results where the structure extends horizontally rather than vertically. In this layout, the individual observations or samples appear along the vertical axis, and the branches stretch left to right, representing how clusters merge step-by-step based on dissimilarity or distance.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11: Ward‚Äôs Linkage (k = 2)\nWard‚Äôs linkage is a hierarchical clustering method focused on minimizing the increase in total within-cluster variance (or error sum of squares) when merging clusters. It is widely used because it tends to produce compact, spherical clusters, making it popular in many applications like biological data, market segmentation, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12: Rectangular Clusters (k = 2)\nRectangular clusters are simply the visual aids drawn as boxes around groups of points identified as clusters on dendrograms or heat maps to facilitate clear, intuitive interpretation of cluster structure in hierarchical clustering outputs.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13: Circular dendrogram (k = 2)\nA Circular Dendrogram is a radial, space-efficient alternative to the classical dendrogram, arranging hierarchical cluster branches in concentric circles around a center. It enhances visual clarity for large or complex datasets by providing a 360-degree view of similarity and cluster structures, making it a valuable tool for hierarchical data visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14: Colored Rectangular Dendrogram (k = 2)\nA coloured rectangular dendrogram is an enhanced dendrogram visualization where clusters are not only enclosed in rectangular boxes but these boxes and the associated branches are also color-coded to clearly distinguish different clusters. This combination improves interpretability and presentation of hierarchical clustering. results.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†15: Classical Dendrogram (k = 2) A classical dendrogram is the traditional tree-like diagram used in hierarchical cluster analysis to represent the nested grouping of objects based on their similarity or distance. It is a fundamental visualization tool that shows how clusters are formed progressively, starting from each individual object and merging step by step into larger clusters until all objects are grouped into one.\n\n\n\n\n\n\n\nIn addition to the various plot types available in RAISINS, the platform also provides several customization options to refine your clustering analysis (See Figure¬†7)\nYou can explore and adjust the following settings:\n\n\n6.2 Linkage Methods\nFollowing linkage methods are available in RAISINS - single, complete, average, ward.D2, mcquitty, median and centroid.\n\n\nLets understand the different linkage methods\n\n\n\n\n\n\n\nNoteüîπ Single linkage\n\n\n\n\n\nSingle linkage defines the distance between two clusters as the minimum distance between any pair of points, one from each cluster. This approach often produces elongated, chain-like clusters because it merges clusters based on the closest individual pair of points. It is sensitive to noise and outliers, as a single close pair can link otherwise distant clusters.\n\n\n\n\n\n\n\n\n\nNoteüîπ Complete linkage\n\n\n\n\n\nComplete linkage measures the distance between two clusters as the maximum distance between any point in one cluster and any point in the other cluster. This method tends to produce more compact and spherical clusters by enforcing that all members within a cluster are close to each other. It is less sensitive to outliers compared to single linkage.\n\n\n\n\n\n\n\n\n\nNoteüîπ Average linkage\n\n\n\n\n\nAverage linkage computes the clustering distance as the average of all pairwise distances between members of the two clusters. It balances the tendency of single and complete linkage, typically yielding clusters that are more balanced in shape and size by considering all members evenly.\n\n\n\n\n\n\n\n\n\nNoteüîπ Ward.D2 Linkage\n\n\n\n\n\nWard‚Äôs linkage merges clusters such that the increase in total within-cluster variance is minimized. It uses a variance-minimizing criterion based on squared Euclidean distances and tends to produce compact, spherical clusters. Ward.D2 specifically refers to Ward‚Äôs method with squared distances, popular for its stable and interpretable clusters.\n\n\n\n\n\n\n\n\n\nNoteüîπ McQuitty Linkage\n\n\n\n\n\nThis method uses a simple average of distances weighted by cluster sizes when merging clusters. It‚Äôs a variant of average linkage and provides a compromise that accounts for different cluster sizes during the merge.\n\n\n\n\n\n\n\n\n\nNoteüîπ Median Linkage\n\n\n\n\n\nMedian linkage calculates the distance between clusters based on the median of the points within each cluster (cluster median) rather than the mean. This method can be more robust to skewed data distributions but may result in non-monotonic clustering (reversals) in dendrograms.\n\n\n\n\n\n\n\n\n\nNoteüîπ Centroid Linkage\n\n\n\n\n\nCentroid linkage computes the distance between two clusters as the Euclidean distance between their centroids (mean vectors). It effectively merges clusters based on the center of mass but may produce inversions (discontinuities) in the dendrogram if the merging decreases cluster distances temporarily..\n\n\n\n\n\n\n6.3 Distance Metrics\nDistance metrics options available in RAISINS: Euclidean, Manhattan, Maximum, Canberra, Minkowski.\n\n\nLets understand the different Distance Metrics\n\n\n\n\n\n\n\nNoteüîπ Euclidean Distance\n\n\n\n\n\nEuclidean distance is the most familiar form of distance measurement, representing the straight-line distance between two points in Euclidean space. It is calculated using the Pythagorean theorem, where the distance between two points is the square root of the sum of the squared differences across all dimensions. For example, in a two-dimensional space, the distance between points (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ) is given by the formula ‚àö((x‚ÇÇ ‚àí x‚ÇÅ)¬≤ + (y‚ÇÇ ‚àí y‚ÇÅ)¬≤). This metric measures the shortest path between points, like stretching a string tight between two points on a map, making it widely used in machine learning, such as clustering and classification algorithms.\n\n\n\n\n\n\n\n\n\nNoteüîπ Manhattan Distance\n\n\n\n\n\nManhattan distance, also known as city block distance, sums the absolute differences of the coordinates between two points in a grid-like path. Unlike Euclidean distance, which measures the shortest ‚Äúas-the-crow-flies‚Äù route, Manhattan distance measures how far apart two points are if you can only move along grid lines‚Äîlike navigating city streets laid out in a grid pattern. For example, the distance between (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ) is |x‚ÇÇ ‚àí x‚ÇÅ| + |y‚ÇÇ ‚àí y‚ÇÅ|, often used in urban planning and in applications where movement is restricted to axes-aligned paths.\n\n\n\n\n\n\n\n\n\nNoteüîπ Maximum Distance\n\n\n\n\n\nMaximum distance, also known as Chebyshev distance, considers the greatest absolute difference among all coordinate pairs between two points. It effectively measures how far apart two points are in terms of the most significant coordinate difference. Mathematically, for points (x‚ÇÅ, y‚ÇÅ) and (x‚ÇÇ, y‚ÇÇ), it is max(|x‚ÇÇ ‚àí x‚ÇÅ|, |y‚ÇÇ ‚àí y‚ÇÅ|). This metric is useful in chess (king‚Äôs move), robotics (max step size), and in certain clustering methods, where the largest coordinate gap dominates the distance calculation.\n\n\n\n\n\n\n\n\n\nNoteüîπ Canberra Distance\n\n\n\n\n\nCanberra distance is a weighted version of difference measurement that emphasizes smaller differences, especially when values are close to zero. It is calculated as the sum of the ratios |x‚ÇÇ ‚àí x‚ÇÅ| / (|x‚ÇÇ| + |x‚ÇÅ|) across all dimensions. Because it gives more weight to differences when values are small, it is effective for datasets with many small or sparse values, often used in bioinformatics, economics, and fields dealing with relative differences.\n\n\n\n\n\n\n\n\n\nNoteüîπ Minkowski Distance\n\n\n\n\n\nMinkowski distance is a generalization of Euclidean and Manhattan distances, characterized by a parameter p, called the order. When p=1, Minkowski becomes Manhattan distance; when p=2, it becomes Euclidean distance. For other values of p, it defines a different form of distance calculation, where higher p emphasizes larger differences more heavily, and lower p emphasizes smaller differences. The formula involves taking the p-th root of the sum of the absolute differences raised to the power p across all dimensions, making it very flexible for various applications.\n\n\n\n\n\n\n\n\n\nNoteüîπ Median and Centroid\n\n\n\n\n\nMedian distance refers to the measure of the difference between data points based on their median values; it is more robust to outliers in skewed data distributions. Centroid distance specifically considers the Euclidean distance between the centroids (mean points) of data clusters. The centroid-based method is widely used in clustering techniques like k-means because it measures the central point of a cluster, making it useful for cluster center-based algorithms but sensitive to outliers, which can shift the centroid.\n\n\n\n\n\n\n6.4 Methods to find the optimal number of clusters (k)\nIn RAISINS, three methods are available to determine the optimal number of clusters - Elbow, Silhouette, and Gap Statistic..\n\n\nLets understand the different methods to find optimal number of clusters\n\n\n\n\n\n\n\nNoteüîπ Elbow Method\n\n\n\n\n\nThe elbow method is a heuristic used in clustering analysis, particularly with K-means, to determine the optimal number of clusters. It involves plotting the within-cluster sum of squares (WCSS) against different values of k (number of clusters). As the number of clusters increases, WCSS decreases because the data points within each cluster become more tightly grouped. The goal is to identify the ‚Äúelbow‚Äù point in the plot where the rate of decrease sharply changes, indicating an optimal balance between minimizing intra-cluster variance and avoiding overfitting. This point suggests the most appropriate number of clusters, where adding more clusters yields diminishing improvements.\n\n\n\n\n\n\n\n\n\nNoteüîπ Silhouette Method\n\n\n\n\n\nThe Silhouette method measures how similar an object is to its own cluster compared to other clusters. For each data point, the Silhouette score ranges from -1 to 1, with higher scores indicating better clustering (points are closer to members of their own cluster than to other clusters). The average silhouette score across all points for different cluster numbers helps identify the optimal k: the higher the average score, the better the clustering. This method considers both cohesion within clusters and separation between clusters, providing a more nuanced evaluation of clustering quality than solely looking at variance.\n\n\n\n\n\n\n\n\n\nNoteüîπ Gap Statistic\n\n\n\n\n\nThe Gap Statistic compares the total within-cluster dispersion for different k values with their expected dispersion under a null reference distribution (usually a uniform distribution of points). It calculates the gap between the observed clustering and the expected clustering under randomness: a larger gap indicates more meaningful, well-separated clusters. The optimal number of clusters is the one where the gap statistic reaches its maximum, suggesting that the clustering structure differs significantly from random noise. This method is robust but more computationally intensive, as it involves generating multiple reference datasets for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†16: Elbow plot\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†17: Silhouette plot\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†18: Gap Statistic\n\n\n\n\n\n\n\n6.4.0.1 Scaling Options available in raisins: zscore, center, minmax, unitlength, robust, none.\n\n\nLets understand the different scaling options\n\n\n\n\n\n\n\nNoteüîπ zscore\n\n\n\n\n\nZ-score normalization, also known as standardization, transforms data so that it has a mean of 0 and a standard deviation of 1. This method computes the Z-score for each data point by subtracting the dataset‚Äôs mean (Œº) and dividing by its standard deviation (œÉ). The resulting scaled data indicates how many standard deviations each point is from the mean. It is particularly useful for datasets with roughly normal distributions and algorithms that assume normality, such as linear regression or SVM, as it removes scale bias and enhances comparability across features.\n\n\n\n\n\n\n\n\n\nNoteüîπ Center\n\n\n\n\n\nCentering data refers to subtracting the mean of a feature from each data point, effectively shifting the data so the mean becomes zero without changing the data‚Äôs variance or distribution shape. This operation is fundamental to many data preprocessing steps, such as Z-score normalization, and helps algorithms interpret features more straightforwardly by aligning data around a common point, making the analysis less affected by location shifts.\n\n\n\n\n\n\n\n\n\nNoteüîπ MinMax\n\n\n\n\n\nMin-Max scaling rescales data to a fixed range, typically. It adjusts each feature by subtracting the minimum value and dividing by the range (max - min), effectively compressing all data within the specified bounds. This technique preserves the original distribution but is highly sensitive to outliers, which can distort the scaled values by expanding the range. Min-Max normalization is often used in neural networks and gradient-based algorithms where feature bounds matter.\n\n\n\n\n\n\n\n\n\nNoteüîπ Unit Length\n\n\n\n\n\nUnit length scaling normalizes each data sample (or feature vector) so that its total length (or Euclidean norm) equals 1. This is achieved by dividing each vector by its magnitude, which is the square root of the sum of squared components. It‚Äôs commonly used in text analysis (e.g., TF-IDF vectors) and machine learning algorithms that rely on cosine similarity, as it ensures all vectors are on the same scale, emphasizing direction over magnitude.\n\n\n\n\n\n\n\n\n\nNoteüîπ Robust\n\n\n\n\n\nRobust scaling uses statistics less sensitive to outliers, typically by subtracting the median and dividing by the interquartile range (IQR). This method centers the data around the median and scales it according to the spread of the middle 50% of data points, making it effective when dealing with noisy data or datasets with extreme outliers. Its primary goal is to produce scaled data that reflects the true distribution of the bulk of the data without being skewed by outliers.\n\n\n\n\n\n\n\n6.5 Heatmap in RAISINS\nA heatmap in cluster analysis is a visualization technique that represents data values in a matrix format using colors to indicate magnitude, typically after both the rows and columns have been reordered based on hierarchical clustering results. The integration of heatmaps with cluster analysis allows you to visually explore and identify patterns, groupings, or similarities among observations (rows) and variables (columns).\nIn the RAISINS platform after uploading your data click on the Heatmap tab to generate the heatmap and you can use the gear icon to customize your heatmap (See Figure¬†19) By default the data are standardized using the zscore scaling method, ensuring all features contribute equally to clustering, which was performed using the complete linkage method and euclidean distance metric, resulting in 2 clusters.You can also download the heatmapin high-quality PNG (300 dpi), TIFF or PDF formats for use in reports or presentations.\n\n\n\n\n\n\nFigure¬†19: How to access the Heatmap tab in RAISINS\n\n\n\nSee Figure¬†20 where blocks of similar colors indicate labels or variables with related profiles, and the accompanying dendrograms illustrate their hierarchical relationships, making it easier to identify groups that share similar characteristics or expression patterns.\n\n\n\n\n\n\nFigure¬†20: Heatmap in RAISINS\n\n\n\n\n\n6.6 Metrics tab\nIn Metrics (See Figure¬†21) tab you can see the Distance matrix, Cluster-wise means, Intra-cluster Statistics and Inter-cluster Statistics tables and you can either copy this file or download these tables in .xlsx or .csv formats for use in reports or presentations.\n\n\n\n\n\n\nFigure¬†21: Metrics tab in RAISINS\n\n\n\nDifferent metrics available in Metric tab is given below.\n\nDistance Matrix (euclidean method) - See Figure¬†22\n\n\n\n\n\n\n\nFigure¬†22: Distance Matrix in RAISINS\n\n\n\nIn hierarchical clustering, the distance matrix represents the pairwise dissimilarities or distances between all observations in the dataset. It quantifies how similar or different each observation is from every other observation and serves as the foundation for grouping observations into clusters. The distance matrix provides a reference for understanding the relative closeness of observations and guides the formation of clusters based on similarity.\n\nCluster means - See Figure¬†23\n\n\n\n\n\n\n\nFigure¬†23: Cluster wise means in RAISINS\n\n\n\nIn hierarchical clustering, the cluster mean represents the average values of all variables for the observations within a cluster, summarizing the central tendency of the cluster and providing a reference point to understand the characteristics of the grouped data. It represents the typical characteristics of that cluster.\n\nIntra-cluster Statistics - See Figure¬†24\n\n\n\n\n\n\n\nFigure¬†24: Intracluster statistics\n\n\n\nIn hierarchical clustering, intra-cluster statistics measure the compactness or similarity of observations within the same cluster. They quantify how closely the members of a cluster resemble each other, often using metrics like the average distance between observations and the cluster mean. These statistics help assess the cohesion of a cluster, indicating how homogeneous or tightly grouped the cluster members are.\n\nInter-cluster Statistics - See Figure¬†25\n\n\n\n\n\n\n\nFigure¬†25: Intercluster statistics\n\n\n\nIn hierarchical clustering, inter-cluster statistics measure the separation or dissimilarity between different clusters. They quantify how distinct or far apart the clusters are from each other, often using metrics like the distance between cluster means or centroids. These statistics help assess the distinctness of clusters, indicating how well the clustering algorithm has separated different groups in the dataset.\n\n\n6.7 HCPC (Hierarchical Clustering on Principal Components)\nHierarchical clustering on principal components (HCPC) is a hybrid approach that combines the strengths of principal component analysis (PCA) and hierarchical clustering. PCA first reduces the dimensionality of the dataset by summarizing correlated variables into a smaller set of uncorrelated components that retain most of the original variance. Then, hierarchical clustering is performed on these principal components instead of the raw variables, which enhances the separation of clusters and reduces noise caused by variable correlations. This method is particularly useful for visualizing and identifying meaningful groupings in complex multivariate datasets, as it simplifies interpretation while preserving the main structure of the data.\n\n\nWhen to use HCPC ?\n\n\nWhen you have a large dataset with many continuous variables and want to reduce dimensionality before clustering to improve interpretability and cluster quality.\nWhen your dataset contains categorical variables, multiple correspondence analysis (MCA) or factor analysis can transform them into continuous principal components suitable for clustering.\nWhen original variables are correlated or noisy, HCPC clusters on principal components that summarize the main variance structure and filter noise, leading to more meaningful and stable clusters.\nWhen you want an integrated approach combining principal component analysis for dimensionality reduction, hierarchical clustering for initial cluster identification, and k-means clustering for refinement.\nWhen exploring multivariate data for identifying natural groupings, patterns, or clusters with clearer separation and visual representation (e.g., factor maps, chord diagrams).\nWhen the goal is simplifying complex multivariate data for easier interpretation and presentation in reports or scientific communication.\n\n\n\n\nDifference between HCPC and classical HCA\n\n\nHCPC applies Principal Component Analysis (PCA) first to reduce data dimensionality before clustering; HCA clusters directly on the original variables.\nHCPC clusters the uncorrelated principal components that represent the main variance structure, filtering noise and redundancy; HCA uses raw data and can be influenced by correlated variables.\nHCPC integrates PCA, hierarchical clustering, and k-means clustering to improve cluster stability and quality; HCA only involves hierarchical clustering.\nHCPC is designed for complex, high-dimensional, or mixed-type datasets; HCA is suitable for simpler, low-dimensional data.\nHCPC produces enhanced visualizations like factor maps and chord diagrams for better cluster interpretation; HCA mainly produces dendrograms based on distances.\nHCPC requires additional computational steps but offers more robust and interpretable clusters; HCA is simpler and faster but may lead to less meaningful clusters in complex data.\n\n\nIn the RAISINS platform after uploading your data select the labels then select the clustering variables of your choice, then click on Run Analysis and go to HCPC to generate Factor map, Chord diagram and Chord diagram - Cluster means which are downloadable in PNG, TIFF or PDF formats with interpretation and tables. The tables can be either copied or downloaded in .xlsx or .csv formats for use in reports or presentations.\n\n\n\n\n\n\n\n\n\nFigure¬†26: Factor Map\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27: Chord diagram\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†28: Chord diagram - Cluster means\n\n\n\n\n\n\nIn the Figure¬†26 the Factor map displays the results of Hierarchical Clustering on Principal Components (HCPC), where observations are grouped into distinct clusters based on their similarity across all variables. The plot is constructed in a reduced two-dimensional space using the first two principal components (Dim1 and Dim2), which capture the maximum variance in the data while preserving the essential structure of the original multidimensional dataset. Each cluster is represented by a different color and enclosed in a convex hull (shaded polygon), making it easy to visually distinguish between groups. Observations within the same cluster share similar characteristics across the measured variables, while observations in different clusters exhibit distinct patterns or profiles. The percentages shown on each axis indicate how much of the total variance in the data is explained by that dimension - higher percentages mean that dimension captures more information about the differences between observations. The spatial separation between clusters reflects how different they are: clusters that are far apart have very different profiles, while clusters that are closer together are more similar. Overlapping regions suggest some ambiguity in cluster boundaries, where observations share characteristics with multiple groups. The position of each observation (data point label) within the plot is determined by its scores on the principal components, allowing you to see not only which cluster each observation belongs to but also how it relates to other observations within and across clusters. This visualization helps identify natural groupings in your data, understand the relationships between observations, and assess the quality of the clustering by examining how well-separated and cohesive the clusters appear in the reduced dimensional space\n\n\n\n\n\n\nFigure¬†29: Top variables\n\n\n\nIn Figure¬†29, the table shows the top contributing variables for each cluster based on the v.test statistic from the HCPC analysis. For each cluster, the five variables with the highest absolute v.test values are displayed. ‚ÄòMean in category‚Äô and ‚Äòsd in category‚Äô show the standardized mean and standard deviation within the cluster, while ‚ÄòOverall mean‚Äô and ‚ÄòOverall sd‚Äô provide reference values across all data. The p.value indicates the statistical significance of the variable‚Äôs contribution. Variables with higher absolute v.test values and lower p.values are more important in defining the cluster.\nIn Figure¬†27, the chord diagram visualizes the relationships between your original variables and the identified clusters from Hierarchical Clustering on Principal Components (HCPC). The outer ring shows two types of segments: your original dataset variables and the clusters (groups of similar observations). The ribbons connecting them represent the strength of association, with wider ribbons indicating that a variable is more characteristic or defining for that particular cluster. The width is determined by v.test statistics, which measure how strongly a variable distinguishes each cluster from others. By examining these connections, you can understand the profile of each cluster - which variables are most prominent in defining each group. Variables with thick ribbons to a cluster are key features of that group, while variables connected to multiple clusters play important roles across different groups. The diagram displays the most characteristic variables for each cluster with weights normalized within each cluster, allowing you to compare the relative importance of variables in defining cluster identities and understand what makes each group unique in your dataset.\n\n\n\n\n\n\n\n\n\nFigure¬†30: HCPC Dendrogram\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†31: Heirarchical Factor Map\n\n\n\n\n\n\nIn Figure¬†30, the hierarchical dendrogram obtained through Hierarchical Clustering on Principal Components (HCPC) provides a visual representation of how observations are grouped based on their similarity in the principal component space. The vertical branches depict the progressive merging of individuals or groups, where shorter branches indicate closer similarity and longer branches represent greater dissimilarity between clusters. The height of each merge on the y-axis corresponds to the distance or dissimilarity at which clusters combine, allowing you to assess how distinct each group is. The colored rectangles highlight the main clusters identified by the analysis, where observations within the same color share similar multivariate characteristics derived from PCA. By examining the branching pattern and the height at which groups join, you can interpret the overall structure and relationships among the observations‚Äîhow closely related they are, which clusters are more homogeneous, and where clear separations exist. This dendrogram thus serves as a hierarchical map of similarity, illustrating the underlying grouping patterns in your dataset as summarized by the principal components.\nIn Figure¬†31, the three-dimensional hierarchical clustering plot projected onto the PCA factor map visually integrates both the dimensional reduction from Principal Component Analysis (PCA) and the grouping structure from Hierarchical Clustering on Principal Components (HCPC). The horizontal plane represents the first two principal components (Dim 1 and Dim 2), which together capture the largest proportion of variance in the dataset, while the vertical axis (‚Äúheight‚Äù) indicates the dissimilarity or distance at which observations or clusters are merged. Each line segment shows how closely related individuals (such as genotypes or samples) are in the multivariate space; shorter vertical distances signify stronger similarity. The clusters, represented by colored labels, highlight groups of observations that share common patterns along the principal component dimensions. This 3D representation allows you to observe not only how clusters are hierarchically formed but also how they are spatially positioned and separated in the PCA space, providing a comprehensive view of both the clustering process and the underlying data structure derived from the principal components.\n\n\n\n\n\n\nFigure¬†32: PCA Summary Table\n\n\n\nIn Figure¬†32, the table summarizes the results of Principal Component Analysis (PCA), showing how much variance each principal component explains in the dataset. The eigenvalue indicates the amount of variance captured by each component, with larger values representing more important components. The percent column shows the proportion of total variance explained by each component, while the cumulative percent shows the running total across components. Components with eigenvalues greater than 1 are typically considered meaningful. This information helps determine how many components are needed to adequately represent the original data and forms the basis for subsequent clustering analysis.\n\n\n\n\n\n\nFigure¬†33: Cluster quality indicators\n\n\n\nIn Figure¬†33, the table summarizes the variance in the dataset with respect to clustering. Total Sum of Squares (Total_SS) represents the overall variability in the data. Within-Cluster Sum of Squares (Within_SS) measures variability of observations inside each cluster. Between-Cluster Sum of Squares (Between_SS) quantifies variability between cluster centers. The Between/Total Ratio indicates the proportion of total variance explained by the differences between clusters. Higher ratios mean better separation between clusters."
  }
]